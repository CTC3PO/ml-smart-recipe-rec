{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VJhmu7ayq2-W"
   },
   "source": [
    "## 1 - Data loading and Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GQL1lih9CouO",
    "outputId": "bf31b45f-889e-4397-caf4-ece654b68aa8"
   },
   "outputs": [],
   "source": [
    "# Install libraries\n",
    "!pip install torch_geometric\n",
    "\n",
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datasets import load_dataset\n",
    "import json\n",
    "import ast\n",
    "import networkx as nx\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from PIL import Image\n",
    "import requests\n",
    "from io import BytesIO\n",
    "import cv2\n",
    "from collections import Counter\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GATConv, SAGEConv, HeteroConv\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.decomposition import NMF, TruncatedSVD\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import umap\n",
    "\n",
    "# Set style for visualizations\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Optional advanced imports (with error handling)\n",
    "try:\n",
    "    import xgboost as xgb\n",
    "    XGB_AVAILABLE = True\n",
    "except ImportError:\n",
    "    XGB_AVAILABLE = False\n",
    "    print(\"XGBoost not available. Install with: pip install xgboost\")\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "9ecf43026b3b4b588d944ddacb6d5ebf",
      "a4b0015c22a7408587f0b506e446e69b",
      "c69a3f06e35f4bbb813bdb04a37d7543",
      "55bfcfbd038f4661b3e618ca6f291ab2",
      "4ff5a1ee06734e269860cab268549168",
      "22c478b870f74824adc3451faed40e23",
      "e8ec358ee5e34958b2e09c5c312b5d09",
      "5cf87ad4159b42c6a120097f905243b3",
      "e75501af479b4c2d8ee294c3952ca4cd",
      "71d55f65e7384705b4d70100adb0b171",
      "78bdf17b658243768b34e2ee03aa40dd",
      "849bd2ef797d41aabc80797a82a78042",
      "03408c1ea3844232b86090142775c212",
      "09bd9687ddfa49dab2559e2eaaab3b28",
      "c98906d9bcd04900945904330a8b0230",
      "7e3366b054cf4df49f94fba40d8f7cd4",
      "23bed68f646f46528ab2a430a0d9a3d1",
      "038837fd0e2446f4917367379ba4a097",
      "f19436e3dc9b437fb1458325f8ace551",
      "980c38ef7d0645ec8ac69fd594d87b98",
      "7de47fa048ee427e800506ed0b110899",
      "1eba0ea384c44856814675d639d2355e",
      "a4b9748c2cb645859fd4e6e376e886b2",
      "faed6e6c71984294a5a5dc4242e93055",
      "ed8a9a1078c042f48f7ff3186c45a9fe",
      "ef60fe0b89e041089d9545b232935c78",
      "be9f5cd2b1724c01891375f54358795c",
      "10407668939d4e27a35fa0928d4a8cba",
      "e6bb9597e53b4ec5a547ce4f91e19c07",
      "86a91cc9344845a3a4a0b51b9e170469",
      "bdbeaa88c76c4cdea870e56e88124264",
      "f9417f455f3740918133d0abf661960f",
      "923164f6eb884ce3884c4a1e038f5254"
     ]
    },
    "id": "v7QXkBJ3CtoA",
    "outputId": "56beefd4-5c53-4102-be5d-cb1f5869b276"
   },
   "outputs": [],
   "source": [
    "def load_and_explore_dataset():\n",
    "    \"\"\"Load the MM-Food-100K dataset and perform initial exploration\"\"\"\n",
    "    print(\"Loading dataset from Hugging Face...\")\n",
    "\n",
    "    # Load the dataset\n",
    "    dataset = load_dataset(\"Codatta/MM-Food-100K\")\n",
    "\n",
    "    # Convert to pandas DataFrame for easier manipulation\n",
    "    df = pd.DataFrame(dataset['train'])\n",
    "\n",
    "    print(f\"Dataset shape: {df.shape}\")\n",
    "    print(\"\\nDataset columns:\")\n",
    "    print(df.columns.tolist())\n",
    "\n",
    "    # Display basic info\n",
    "    print(\"\\nDataset info:\")\n",
    "    print(df.info())\n",
    "\n",
    "    # Display first few rows\n",
    "    print(\"\\nFirst 5 rows:\")\n",
    "    display(df.head())\n",
    "\n",
    "    return df\n",
    "\n",
    "# Load the dataset\n",
    "df_raw = load_and_explore_dataset()\n",
    "\n",
    "def initial_data_analysis(df):\n",
    "    \"\"\"Perform initial data analysis\"\"\"\n",
    "    print(\"=\"*50)\n",
    "    print(\"INITIAL DATA ANALYSIS\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "    # Check for missing values\n",
    "    print(\"\\nMissing values per column:\")\n",
    "    missing_values = df.isnull().sum()\n",
    "    print(missing_values[missing_values > 0])\n",
    "\n",
    "    # Check food type distribution\n",
    "    print(\"\\nFood type distribution:\")\n",
    "    food_type_counts = df['food_type'].value_counts()\n",
    "    print(food_type_counts)\n",
    "\n",
    "    # Basic nutritional statistics\n",
    "    print(\"\\nNutritional statistics:\")\n",
    "    # Extract nutritional information from JSON\n",
    "    nutrition_cols = ['calories_kcal', 'protein_g', 'fat_g', 'carbohydrate_g']\n",
    "\n",
    "    # Function to extract nutrition values\n",
    "    def extract_nutrition(nutrition_json, key):\n",
    "        try:\n",
    "            if pd.isna(nutrition_json):\n",
    "                return np.nan\n",
    "            nutrition_dict = json.loads(nutrition_json.replace(\"'\", \"\\\"\"))\n",
    "            return nutrition_dict.get(key, np.nan)\n",
    "        except:\n",
    "            return np.nan\n",
    "\n",
    "    for col in nutrition_cols:\n",
    "        df[col] = df['nutritional_profile'].apply(lambda x: extract_nutrition(x, col))\n",
    "\n",
    "    # Display basic stats for nutritional values\n",
    "    print(df[nutrition_cols].describe())\n",
    "\n",
    "    return df\n",
    "\n",
    "# Perform initial analysis\n",
    "df = initial_data_analysis(df_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SEac6bQHv5Dw"
   },
   "source": [
    "### 1B. data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bZvskbMLtOGJ",
    "outputId": "5284e19a-3144-4889-9b3b-7e5099d7a16d"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "import ast\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def comprehensive_data_cleaning(df):\n",
    "    \"\"\"\n",
    "    Comprehensive data cleaning and wrangling pipeline for MM-Food-100K dataset\n",
    "    \"\"\"\n",
    "    print(\"=\"*60)\n",
    "    print(\"COMPREHENSIVE DATA CLEANING & WRANGLING\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    # Create a copy to avoid modifying the original\n",
    "    df_clean = df.copy()\n",
    "\n",
    "    # Function to extract nutrition values safely\n",
    "    def extract_nutrition(nutrition_json, key):\n",
    "        try:\n",
    "            if pd.isna(nutrition_json):\n",
    "                return np.nan\n",
    "            nutrition_dict = json.loads(nutrition_json.replace(\"'\", \"\\\"\"))\n",
    "            return nutrition_dict.get(key, np.nan)\n",
    "        except:\n",
    "            return np.nan\n",
    "\n",
    "    # Extract nutritional information from JSON first\n",
    "    nutrition_cols = ['calories_kcal', 'protein_g', 'fat_g', 'carbohydrate_g']\n",
    "    for col in nutrition_cols:\n",
    "        df_clean[col] = df_clean['nutritional_profile'].apply(lambda x: extract_nutrition(x, col))\n",
    "\n",
    "\n",
    "    # 1. Handle Missing Values\n",
    "    print(\"1. Handling missing values...\")\n",
    "\n",
    "    # Check missing values\n",
    "    missing_percent = (df_clean.isnull().sum() / len(df_clean)) * 100\n",
    "    print(\"Missing values percentage:\")\n",
    "    print(missing_percent[missing_percent > 0].sort_values(ascending=False))\n",
    "\n",
    "    # Strategy for different columns\n",
    "    missing_strategies = {\n",
    "        # Nutritional data: impute with median by food_type\n",
    "        'calories_kcal': 'median_by_type',\n",
    "        'protein_g': 'median_by_type',\n",
    "        'fat_g': 'median_by_type',\n",
    "        'carbohydrate_g': 'median_by_type',\n",
    "\n",
    "        # Text data: fill with appropriate defaults\n",
    "        'ingredients': 'empty_list', # This column will be replaced by 'ingredients_cleaned'\n",
    "        'cooking_methods': 'unknown', # This column will be replaced by 'cooking_methods_cleaned'\n",
    "        'portion_sizes': 'empty_list', # This column will be replaced by 'portion_weights' and 'total_weight_g'\n",
    "\n",
    "        # Other columns\n",
    "        'dish_name': 'unknown_dish',\n",
    "        'food_type': 'unknown_type'\n",
    "    }\n",
    "\n",
    "    # Apply missing value strategies\n",
    "    for col, strategy in missing_strategies.items():\n",
    "        if col in df_clean.columns and df_clean[col].isnull().sum() > 0:\n",
    "            if strategy == 'median_by_type':\n",
    "                # Impute with median of the same food_type\n",
    "                df_clean[col] = df_clean.groupby('food_type')[col].transform(\n",
    "                    lambda x: x.fillna(x.median()) if x.notnull().sum() > 0 else x.fillna(0)\n",
    "                )\n",
    "            elif strategy == 'empty_list':\n",
    "                df_clean[col] = df_clean[col].fillna('[]')\n",
    "            elif strategy == 'unknown':\n",
    "                df_clean[col] = df_clean[col].fillna('unknown')\n",
    "            elif strategy == 'unknown_dish':\n",
    "                df_clean[col] = df_clean[col].fillna('unknown_dish')\n",
    "            elif strategy == 'unknown_type':\n",
    "                df_clean[col] = df_clean[col].fillna('unknown_type')\n",
    "\n",
    "    # 2. Data Type Conversion and Validation\n",
    "    print(\"\\n2. Data type conversion and validation...\")\n",
    "\n",
    "    # Convert nutritional columns to numeric, handling errors\n",
    "    nutrition_cols = ['calories_kcal', 'protein_g', 'fat_g', 'carbohydrate_g']\n",
    "    for col in nutrition_cols:\n",
    "        df_clean[col] = pd.to_numeric(df_clean[col], errors='coerce')\n",
    "        # Fill any remaining NaN with median after coercion\n",
    "        df_clean[col] = df_clean[col].fillna(df_clean[col].median())\n",
    "\n",
    "    # 3. Ingredient List Processing\n",
    "    print(\"\\n3. Processing ingredient lists...\")\n",
    "\n",
    "    def clean_ingredient_list(ingredient_str):\n",
    "        \"\"\"Clean and standardize ingredient lists\"\"\"\n",
    "        try:\n",
    "            if pd.isna(ingredient_str) or ingredient_str == '[]' or ingredient_str is None:\n",
    "                return []\n",
    "\n",
    "            # Handle different list formats\n",
    "            if isinstance(ingredient_str, str):\n",
    "                # Clean the string\n",
    "                ingredient_str = ingredient_str.strip()\n",
    "                if ingredient_str.startswith('[') and ingredient_str.endswith(']'):\n",
    "                    ingredients = ast.literal_eval(ingredient_str)\n",
    "                else:\n",
    "                    # Handle malformed lists\n",
    "                    ingredients = [ing.strip() for ing in ingredient_str.split(',')]\n",
    "            elif isinstance(ingredient_str, list):\n",
    "                 ingredients = ingredient_str\n",
    "            else:\n",
    "                 return []\n",
    "\n",
    "\n",
    "            # Clean each ingredient\n",
    "            cleaned_ingredients = []\n",
    "            for ingredient in ingredients:\n",
    "                if isinstance(ingredient, str):\n",
    "                    # Standardize formatting\n",
    "                    ing = ingredient.lower().strip()\n",
    "                    # Remove common prefixes/suffixes\n",
    "                    ing = re.sub(r'^\\d+\\s*', '', ing)  # Remove quantities like \"2 \"\n",
    "                    ing = re.sub(r'\\s*\\(.*\\)', '', ing)  # Remove parentheses content\n",
    "                    ing = re.sub(r'\\s*(tbsp|tsp|cup|cups|oz|lb|lbs|g|kg|ml|l)$', '', ing)  # Remove units\n",
    "                    ing = ing.strip()\n",
    "\n",
    "                    if ing and len(ing) > 1:  # Filter out empty/single character ingredients\n",
    "                        cleaned_ingredients.append(ing)\n",
    "\n",
    "            return list(set(cleaned_ingredients))  # Remove duplicates\n",
    "\n",
    "        except (ValueError, SyntaxError, TypeError) as e:\n",
    "            print(f\"Error processing ingredients: {e}\")\n",
    "            return []\n",
    "\n",
    "    df_clean['ingredients_cleaned'] = df_clean['ingredients'].apply(clean_ingredient_list)\n",
    "\n",
    "    # 4. Cooking Methods Processing\n",
    "    print(\"\\n4. Processing cooking methods...\")\n",
    "\n",
    "    def clean_cooking_methods(method_str):\n",
    "        \"\"\"Clean and standardize cooking methods\"\"\"\n",
    "        try:\n",
    "            if pd.isna(method_str) or method_str is None or method_str == 'unknown':\n",
    "                return []\n",
    "\n",
    "            if isinstance(method_str, str):\n",
    "                methods = [m.strip().lower() for m in method_str.split(',')]\n",
    "                methods = [m for m in methods if m and m != 'unknown']\n",
    "                return list(set(methods))  # Remove duplicates\n",
    "            elif isinstance(method_str, list):\n",
    "                 methods = method_str\n",
    "                 methods = [m.strip().lower() for m in methods if isinstance(m, str)]\n",
    "                 methods = [m for m in methods if m and m != 'unknown']\n",
    "                 return list(set(methods))\n",
    "            else:\n",
    "                return []\n",
    "\n",
    "        except (AttributeError, TypeError) as e:\n",
    "            print(f\"Error processing cooking methods: {e}\")\n",
    "            return []\n",
    "\n",
    "    df_clean['cooking_methods_cleaned'] = df_clean['cooking_method'].apply(clean_cooking_methods)\n",
    "\n",
    "    # 5. Portion Size Processing\n",
    "    print(\"\\n5. Processing portion sizes...\")\n",
    "\n",
    "    def extract_portion_weights(portion_str):\n",
    "        \"\"\"Extract weights from portion size information\"\"\"\n",
    "        try:\n",
    "            if pd.isna(portion_str) or portion_str is None or portion_str == '[]':\n",
    "                return [], 0\n",
    "\n",
    "            if isinstance(portion_str, str):\n",
    "                if portion_str.startswith('[') and portion_str.endswith(']'):\n",
    "                    portions = ast.literal_eval(portion_str)\n",
    "                else:\n",
    "                    portions = [p.strip() for p in portion_str.split(',')]\n",
    "            elif isinstance(portion_str, list):\n",
    "                 portions = portion_str\n",
    "            else:\n",
    "                 return [], 0\n",
    "\n",
    "\n",
    "            weights = []\n",
    "            total_weight = 0\n",
    "\n",
    "            for portion in portions:\n",
    "                if isinstance(portion, str) and ':' in portion:\n",
    "                    try:\n",
    "                        # Extract weight value\n",
    "                        weight_part = portion.split(':')[1].strip()\n",
    "                        # Remove units and convert to float\n",
    "                        weight_value = re.sub(r'[^\\d.]', '', weight_part)\n",
    "                        if weight_value:\n",
    "                            weight_float = float(weight_value)\n",
    "                            weights.append(weight_float)\n",
    "                            total_weight += weight_float\n",
    "                    except (ValueError, IndexError):\n",
    "                        continue\n",
    "\n",
    "            return weights, total_weight\n",
    "\n",
    "        except (ValueError, SyntaxError, TypeError) as e:\n",
    "            print(f\"Error processing portion sizes: {e}\")\n",
    "            return [], 0\n",
    "\n",
    "    portion_results = df_clean['portion_size'].apply(extract_portion_weights)\n",
    "    df_clean['portion_weights'] = portion_results.apply(lambda x: x[0])\n",
    "    df_clean['total_weight_g'] = portion_results.apply(lambda x: x[1])\n",
    "\n",
    "    # 6. Outlier Detection and Handling\n",
    "    print(\"\\n6. Handling outliers...\")\n",
    "\n",
    "    def detect_outliers_iqr(series, threshold=1.5):\n",
    "        \"\"\"Detect outliers using IQR method\"\"\"\n",
    "        Q1 = series.quantile(0.25)\n",
    "        Q3 = series.quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        lower_bound = Q1 - threshold * IQR\n",
    "        upper_bound = Q3 + threshold * IQR\n",
    "        return (series < lower_bound) | (series > upper_bound)\n",
    "\n",
    "    # Detect outliers in nutritional values\n",
    "    outlier_cols = ['calories_kcal', 'protein_g', 'fat_g', 'carbohydrate_g', 'total_weight_g']\n",
    "    for col in outlier_cols:\n",
    "        if col in df_clean.columns:\n",
    "            outliers = detect_outliers_iqr(df_clean[col].dropna())\n",
    "            print(f\"Outliers in {col}: {outliers.sum()} ({outliers.mean()*100:.2f}%)\")\n",
    "\n",
    "            # Cap outliers (winsorization)\n",
    "            if outliers.sum() > 0:\n",
    "                # Using 1st and 99th percentiles as capping values\n",
    "                lower_bound = df_clean[col].quantile(0.01)\n",
    "                upper_bound = df_clean[col].quantile(0.99)\n",
    "                df_clean[col] = df_clean[col].clip(lower_bound, upper_bound)\n",
    "\n",
    "    # 7. Text Data Cleaning\n",
    "    print(\"\\n7. Cleaning text data...\")\n",
    "\n",
    "    def clean_dish_name(name):\n",
    "        \"\"\"Clean and standardize dish names\"\"\"\n",
    "        if pd.isna(name) or name is None or name == 'unknown_dish':\n",
    "            return 'unknown_dish'\n",
    "\n",
    "        name = str(name).strip().lower()\n",
    "        # Remove extra spaces and special characters\n",
    "        name = re.sub(r'[^\\w\\s]', ' ', name)\n",
    "        name = re.sub(r'\\s+', ' ', name)\n",
    "        return name.strip()\n",
    "\n",
    "    df_clean['dish_name'] = df_clean['dish_name'].apply(clean_dish_name)\n",
    "\n",
    "    # 8. Food Type Standardization\n",
    "    print(\"\\n8. Standardizing food types...\")\n",
    "\n",
    "    def standardize_food_type(food_type):\n",
    "        \"\"\"Standardize food type categories\"\"\"\n",
    "        if pd.isna(food_type) or food_type is None or food_type == 'unknown_type':\n",
    "            return 'unknown'\n",
    "\n",
    "        food_type = str(food_type).lower().strip()\n",
    "\n",
    "        # Standardize categories\n",
    "        type_mapping = {\n",
    "            'homemade': 'homemade',\n",
    "            'home made': 'homemade',\n",
    "            'home-made': 'homemade',\n",
    "            'restaurant': 'restaurant',\n",
    "            'restaurant food': 'restaurant',\n",
    "            'raw': 'raw_vegetables_fruits',\n",
    "            'raw vegetables': 'raw_vegetables_fruits',\n",
    "            'raw fruits': 'raw_vegetables_fruits',\n",
    "            'vegetables': 'raw_vegetables_fruits',\n",
    "            'fruits': 'raw_vegetables_fruits',\n",
    "            'packaged': 'packaged_food',\n",
    "            'packaged food': 'packaged_food',\n",
    "            'processed': 'packaged_food'\n",
    "        }\n",
    "\n",
    "        return type_mapping.get(food_type, food_type)\n",
    "\n",
    "    df_clean['food_type_standardized'] = df_clean['food_type'].apply(standardize_food_type)\n",
    "\n",
    "    # 9. Feature Engineering Preparation\n",
    "    print(\"\\n9. Preparing for feature engineering...\")\n",
    "\n",
    "    # Create flags for data quality\n",
    "    df_clean['has_nutrition_data'] = (\n",
    "        (df_clean['calories_kcal'].notna()) &\n",
    "        (df_clean['protein_g'].notna()) &\n",
    "        (df_clean['fat_g'].notna()) &\n",
    "        (df_clean['carbohydrate_g'].notna())\n",
    "    )\n",
    "\n",
    "    df_clean['has_ingredients'] = df_clean['ingredients_cleaned'].apply(lambda x: len(x) > 0)\n",
    "    df_clean['has_cooking_methods'] = df_clean['cooking_methods_cleaned'].apply(lambda x: len(x) > 0)\n",
    "    df_clean['has_portion_data'] = df_clean['total_weight_g'].notna() & (df_clean['total_weight_g'] > 0)\n",
    "\n",
    "    # Calculate data completeness score\n",
    "    df_clean['data_quality_score'] = (\n",
    "        df_clean['has_nutrition_data'].astype(int) +\n",
    "        df_clean['has_ingredients'].astype(int) +\n",
    "        df_clean['has_cooking_methods'].astype(int) +\n",
    "        df_clean['has_portion_data'].astype(int)\n",
    "    ) / 4\n",
    "\n",
    "    # 10. Final Data Quality Report\n",
    "    print(\"\\n10. Final data quality report:\")\n",
    "\n",
    "    quality_report = {\n",
    "        'total_records': len(df_clean),\n",
    "        'records_with_complete_nutrition': df_clean['has_nutrition_data'].sum(),\n",
    "        'records_with_ingredients': df_clean['has_ingredients'].sum(),\n",
    "        'records_with_cooking_methods': df_clean['has_cooking_methods'].sum(),\n",
    "        'records_with_portion_data': df_clean['has_portion_data'].sum(),\n",
    "        'records_with_high_quality': (df_clean['data_quality_score'] >= 0.75).sum(),\n",
    "        'average_data_quality_score': df_clean['data_quality_score'].mean()\n",
    "    }\n",
    "\n",
    "    for metric, value in quality_report.items():\n",
    "        if 'average' in metric:\n",
    "            print(f\"{metric}: {value:.3f}\")\n",
    "        else:\n",
    "            print(f\"{metric}: {value}\")\n",
    "\n",
    "    # 11. Export cleaned data\n",
    "    print(\"\\n11. Exporting cleaned data...\")\n",
    "\n",
    "    # Select only the cleaned columns for further analysis\n",
    "    clean_columns = [\n",
    "        'dish_name', 'food_type_standardized', 'calories_kcal',\n",
    "        'protein_g', 'fat_g', 'carbohydrate_g', 'ingredients_cleaned',\n",
    "        'cooking_methods_cleaned', 'portion_weights', 'total_weight_g',\n",
    "        'has_nutrition_data', 'has_ingredients', 'has_cooking_methods',\n",
    "        'has_portion_data', 'data_quality_score'\n",
    "    ]\n",
    "\n",
    "    # Keep original columns that are already clean or needed for other steps\n",
    "    original_clean_cols = [col for col in df.columns if col not in [\n",
    "        'dish_name', 'food_type', 'nutritional_profile', 'ingredients',\n",
    "        'cooking_method', 'portion_size' # Corrected 'cooking_methods' and 'portion_sizes'\n",
    "    ]]\n",
    "\n",
    "    # Ensure 'sub_dt', 'image_url', 'camera_or_phone_prob', 'food_prob' are included\n",
    "    essential_original_cols = ['sub_dt', 'image_url', 'camera_or_phone_prob', 'food_prob']\n",
    "    for col in essential_original_cols:\n",
    "      if col not in original_clean_cols:\n",
    "        original_clean_cols.append(col)\n",
    "\n",
    "\n",
    "    final_columns = clean_columns + original_clean_cols\n",
    "    # Filter out columns that don't exist in df_clean\n",
    "    df_final = df_clean[[col for col in final_columns if col in df_clean.columns]]\n",
    "\n",
    "    # Save cleaned dataset\n",
    "    df_final.to_csv('mm_food_100k_cleaned.csv', index=False)\n",
    "    print(\"Cleaned dataset saved to 'mm_food_100k_cleaned.csv'\")\n",
    "\n",
    "    return df_final\n",
    "\n",
    "# Enhanced load function with comprehensive cleaning\n",
    "def load_and_clean_dataset():\n",
    "    \"\"\"\n",
    "    Load dataset and apply comprehensive cleaning\n",
    "    \"\"\"\n",
    "    print(\"Loading and cleaning MM-Food-100K dataset...\")\n",
    "\n",
    "    # Load the dataset\n",
    "    dataset = load_dataset(\"Codatta/MM-Food-100K\")\n",
    "    df = pd.DataFrame(dataset['train'])\n",
    "\n",
    "    print(f\"Original dataset shape: {df.shape}\")\n",
    "\n",
    "    # Apply comprehensive cleaning\n",
    "    df_clean = comprehensive_data_cleaning(df)\n",
    "\n",
    "    print(f\"Cleaned dataset shape: {df_clean.shape}\")\n",
    "    print(\"\\nCleaned dataset info:\")\n",
    "    print(df_clean.info())\n",
    "\n",
    "    return df_clean\n",
    "\n",
    "# Load and clean the dataset\n",
    "df = load_and_clean_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lmDfUUbCq8LX"
   },
   "source": [
    "## 2 - EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XtEXubwi0vJM"
   },
   "source": [
    "### **Objective**\n",
    " This comprehensive EDA provides a solid foundation for understanding the cleaned dataset's characteristics, relationships, and patterns, which is essential for effective feature engineering and model building.\n",
    "\n",
    "**The analysis reveals:**\n",
    "\n",
    "    \n",
    "\n",
    "*    Clear relationships between ingredients, cooking methods, and nutritional\n",
    "*   Patterns in food preparation complexity\n",
    "* Insights into dietary characteristics across different food types\n",
    "*Data quality assessment for reliable modeling\n",
    "\n",
    "### **Key Data EDA Steps:**\n",
    "\n",
    "**1. Data Quality**\n",
    "\n",
    "   *  Missing values properly handled\n",
    "\n",
    "   *  Consistent data types and formats\n",
    "\n",
    "*   Outliers identified and treated\n",
    "\n",
    "**2. Nutritional Patterns**\n",
    "\n",
    "   *  Macronutrient distributions and correlations\n",
    "\n",
    "* Energy density analysis\n",
    "\n",
    "*  Portion size characteristics\n",
    "\n",
    "**3. Ingredient Relationships**\n",
    "\n",
    "*     Most common ingredients and their frequencies\n",
    "\n",
    "*     Relationship between ingredient count and nutrition\n",
    "\n",
    "*     Ingredient diversity patterns\n",
    "\n",
    "**4. Cooking Method Impact**\n",
    "\n",
    " *    Most frequently used cooking techniques\n",
    "\n",
    "*  Nutritional differences between cooking methods\n",
    "\n",
    "*     Preparation time analysis\n",
    "\n",
    "**5. Advanced Metrics**\n",
    "\n",
    "*     Macronutrient ratios\n",
    "\n",
    "*     Data quality scores\n",
    "\n",
    "*     Complexity measures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "QIkGJH1CDGhW",
    "outputId": "06a13f9a-d2ea-4f54-9f15-17b7e55efe75"
   },
   "outputs": [],
   "source": [
    "def comprehensive_eda_cleaned(df):\n",
    "    \"\"\"\n",
    "    Comprehensive EDA using the cleaned dataset\n",
    "    \"\"\"\n",
    "    print(\"=\"*60)\n",
    "    print(\"COMPREHENSIVE EDA WITH CLEANED DATA\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    # 1. Dataset Overview\n",
    "    print(\"1. DATASET OVERVIEW\")\n",
    "    print(f\"Dataset shape: {df.shape}\")\n",
    "    print(f\"Columns: {list(df.columns)}\")\n",
    "    print(f\"Data types:\\n{df.dtypes.value_counts()}\")\n",
    "\n",
    "    # Check for remaining missing values\n",
    "    missing_values = df.isnull().sum()\n",
    "    missing_percent = (missing_values / len(df)) * 100\n",
    "    print(f\"\\nMissing values:\\n{missing_values[missing_values > 0]}\")\n",
    "    print(f\"Missing percentage:\\n{missing_percent[missing_percent > 0].round(2)}\")\n",
    "\n",
    "    # 2. Basic Statistics\n",
    "    print(\"\\n2. BASIC STATISTICS\")\n",
    "\n",
    "    # Nutritional statistics\n",
    "    nutrition_cols = ['calories_kcal', 'protein_g', 'fat_g', 'carbohydrate_g', 'total_weight_g']\n",
    "    print(\"Nutritional statistics:\")\n",
    "    print(df[nutrition_cols].describe().round(2))\n",
    "\n",
    "    # Ingredient statistics\n",
    "    df['num_ingredients'] = df['ingredients_cleaned'].apply(len)\n",
    "    df['num_cooking_methods'] = df['cooking_methods_cleaned'].apply(len)\n",
    "\n",
    "    print(f\"\\nAverage ingredients per recipe: {df['num_ingredients'].mean():.2f}\")\n",
    "    print(f\"Average cooking methods per recipe: {df['num_cooking_methods'].mean():.2f}\")\n",
    "    # Removed the line referencing 'estimated_prep_time'\n",
    "    # print(f\"Average preparation time: {df['estimated_prep_time'].mean():.2f} minutes\")\n",
    "\n",
    "    # 3. Food Type Analysis\n",
    "    print(\"\\n3. FOOD TYPE ANALYSIS\")\n",
    "\n",
    "    if 'food_type_standardized' in df.columns:\n",
    "        food_type_counts = df['food_type_standardized'].value_counts()\n",
    "        print(\"Food type distribution:\")\n",
    "        for food_type, count in food_type_counts.items():\n",
    "            print(f\"  {food_type}: {count} ({count/len(df)*100:.1f}%)\")\n",
    "\n",
    "        # Nutritional comparison by food type\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "        for i, col in enumerate(nutrition_cols[:4]):\n",
    "            row, col_idx = i // 2, i % 2\n",
    "            sns.boxplot(data=df, x='food_type_standardized', y=col, ax=axes[row, col_idx])\n",
    "            axes[row, col_idx].set_title(f'{col} by Food Type')\n",
    "            axes[row, col_idx].tick_params(axis='x', rotation=45)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    # 4. Ingredient Analysis\n",
    "    print(\"\\n4. INGREDIENT ANALYSIS\")\n",
    "\n",
    "    # Get all ingredients\n",
    "    all_ingredients = [ingredient for sublist in df['ingredients_cleaned'] for ingredient in sublist]\n",
    "    ingredient_counts = Counter(all_ingredients)\n",
    "\n",
    "    print(f\"Total unique ingredients: {len(ingredient_counts)}\")\n",
    "    print(f\"Total ingredient instances: {len(all_ingredients)}\")\n",
    "\n",
    "    # Top ingredients\n",
    "    top_ingredients = ingredient_counts.most_common(20)\n",
    "    print(\"\\nTop 20 ingredients:\")\n",
    "    for ingredient, count in top_ingredients:\n",
    "        print(f\"  {ingredient}: {count}\")\n",
    "\n",
    "    # Plot top ingredients\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    ingredients, counts = zip(*top_ingredients)\n",
    "    sns.barplot(x=list(counts), y=list(ingredients))\n",
    "    plt.title('Top 20 Most Common Ingredients')\n",
    "    plt.xlabel('Count')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # 5. Cooking Methods Analysis\n",
    "    print(\"\\n5. COOKING METHODS ANALYSIS\")\n",
    "\n",
    "    # Get all cooking methods\n",
    "    all_methods = [method for sublist in df['cooking_methods_cleaned'] for method in sublist]\n",
    "    method_counts = Counter(all_methods)\n",
    "\n",
    "    print(f\"Total unique cooking methods: {len(method_counts)}\")\n",
    "    print(f\"Total method instances: {len(all_methods)}\")\n",
    "\n",
    "    # Top cooking methods\n",
    "    top_methods = method_counts.most_common(15)\n",
    "    print(\"\\nTop 15 cooking methods:\")\n",
    "    for method, count in top_methods:\n",
    "        print(f\"  {method}: {count}\")\n",
    "\n",
    "    # Plot cooking methods\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    methods, counts = zip(*top_methods)\n",
    "    sns.barplot(x=list(counts), y=list(methods))\n",
    "    plt.title('Top 15 Cooking Methods')\n",
    "    plt.xlabel('Count')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # 6. Nutritional Analysis\n",
    "    print(\"\\n6. NUTRITIONAL ANALYSIS\")\n",
    "\n",
    "    # Distribution of nutritional values\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "\n",
    "    # Check if 'nutritional_density' exists before adding to list\n",
    "    nutrition_cols_extended = nutrition_cols.copy()\n",
    "    if 'nutritional_density' in df.columns:\n",
    "        nutrition_cols_extended.append('nutritional_density')\n",
    "\n",
    "    for i, col in enumerate(nutrition_cols_extended):\n",
    "        row, col_idx = i // 3, i % 3\n",
    "        if col in df.columns:\n",
    "            sns.histplot(df[col].dropna(), bins=50, ax=axes[row, col_idx], kde=True)\n",
    "            axes[row, col_idx].set_title(f'Distribution of {col}')\n",
    "            axes[row, col_idx].set_xlabel(col)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Correlation matrix\n",
    "    # Ensure all columns exist before calculating correlation\n",
    "    cols_for_corr = [col for col in nutrition_cols_extended if col in df.columns]\n",
    "    if len(cols_for_corr) > 1:\n",
    "        nutritional_corr = df[cols_for_corr].corr()\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        sns.heatmap(nutritional_corr, annot=True, cmap='coolwarm', center=0, fmt='.2f')\n",
    "        plt.title('Nutritional Values Correlation Matrix')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"\\nInsufficient nutritional columns for correlation matrix.\")\n",
    "\n",
    "\n",
    "    # 7. Portion Size Analysis\n",
    "    print(\"\\n7. PORTION SIZE ANALYSIS\")\n",
    "\n",
    "    if 'total_weight_g' in df.columns:\n",
    "        print(f\"Average portion size: {df['total_weight_g'].mean():.2f}g\")\n",
    "        print(f\"Median portion size: {df['total_weight_g'].median():.2f}g\")\n",
    "        print(f\"Portion size range: {df['total_weight_g'].min():.2f}g - {df['total_weight_g'].max():.2f}g\")\n",
    "\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        sns.histplot(df['total_weight_g'].dropna(), bins=50, kde=True)\n",
    "        plt.title('Distribution of Portion Sizes')\n",
    "        plt.xlabel('Weight (g)')\n",
    "        plt.ylabel('Count')\n",
    "        plt.show()\n",
    "\n",
    "    # 8. Relationship: Ingredients vs Nutrition\n",
    "    print(\"\\n8. INGREDIENTS vs NUTRITION RELATIONSHIP\")\n",
    "\n",
    "    # Scatter plots: Number of ingredients vs nutritional values\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    for i, col in enumerate(nutrition_cols[:4]):\n",
    "        row, col_idx = i // 2, i % 2\n",
    "        sns.scatterplot(data=df, x='num_ingredients', y=col, alpha=0.6, ax=axes[row, col_idx])\n",
    "        axes[row, col_idx].set_title(f'{col} vs Number of Ingredients')\n",
    "        axes[row, col_idx].set_xlabel('Number of Ingredients')\n",
    "        axes[row, col_idx].set_ylabel(col)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Correlation analysis\n",
    "    ingredient_nutrition_corr = df[['num_ingredients'] + nutrition_cols].corr()\n",
    "    print(\"Correlation between ingredient count and nutrition:\")\n",
    "    print(ingredient_nutrition_corr['num_ingredients'][1:].round(3))\n",
    "\n",
    "    # 9. Relationship: Cooking Methods vs Nutrition\n",
    "    print(\"\\n9. COOKING METHODS vs NUTRITION RELATIONSHIP\")\n",
    "\n",
    "    # Analyze nutritional impact of cooking methods\n",
    "    top_methods_list = [method for method, count in top_methods[:8]]\n",
    "    method_nutrition = {}\n",
    "\n",
    "    for method in top_methods_list:\n",
    "        method_recipes = df[df['cooking_methods_cleaned'].apply(lambda x: method in x)]\n",
    "        if len(method_recipes) > 10:\n",
    "            method_nutrition[method] = {\n",
    "                'count': len(method_recipes),\n",
    "                'avg_calories': method_recipes['calories_kcal'].mean(),\n",
    "                'avg_protein': method_recipes['protein_g'].mean(),\n",
    "                'avg_fat': method_recipes['fat_g'].mean(),\n",
    "                'avg_carbs': method_recipes['carbohydrate_g'].mean()\n",
    "            }\n",
    "\n",
    "    # Create comparison plot\n",
    "    methods = list(method_nutrition.keys())\n",
    "    calories = [method_nutrition[m]['avg_calories'] for m in methods]\n",
    "    protein = [method_nutrition[m]['avg_protein'] for m in methods]\n",
    "    fat = [method_nutrition[m]['avg_fat'] for m in methods]\n",
    "    carbs = [method_nutrition[m]['avg_carbs'] for m in methods]\n",
    "\n",
    "    x = np.arange(len(methods))\n",
    "    width = 0.2\n",
    "\n",
    "    plt.figure(figsize=(14, 8))\n",
    "    plt.bar(x - width*1.5, calories, width, label='Calories', alpha=0.8)\n",
    "    plt.bar(x - width/2, protein, width, label='Protein (g)', alpha=0.8)\n",
    "    plt.bar(x + width/2, fat, width, label='Fat (g)', alpha=0.8)\n",
    "    plt.bar(x + width*1.5, carbs, width, label='Carbs (g)', alpha=0.8)\n",
    "\n",
    "    plt.xlabel('Cooking Methods')\n",
    "    plt.ylabel('Nutritional Values')\n",
    "    plt.title('Nutritional Profile by Cooking Method')\n",
    "    plt.xticks(x, methods, rotation=45)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # 10. Advanced Nutritional Analysis\n",
    "    print(\"\\n10. ADVANCED NUTRITIONAL ANALYSIS\")\n",
    "\n",
    "    # Macronutrient ratios\n",
    "    # Check if columns exist before calculating ratios\n",
    "    if all(col in df.columns for col in nutrition_cols):\n",
    "        total_macros = df['protein_g'] + df['fat_g'] + df['carbohydrate_g']\n",
    "        df['protein_ratio'] = df['protein_g'] / total_macros.replace(0, 1)\n",
    "        df['fat_ratio'] = df['fat_g'] / total_macros.replace(0, 1)\n",
    "        df['carb_ratio'] = df['carbohydrate_g'] / total_macros.replace(0, 1)\n",
    "\n",
    "        # Plot macronutrient distribution\n",
    "        macro_ratios = ['protein_ratio', 'fat_ratio', 'carb_ratio']\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        for ratio in macro_ratios:\n",
    "            sns.kdeplot(df[ratio].dropna(), label=ratio.replace('_ratio', '').title())\n",
    "        plt.title('Distribution of Macronutrient Ratios')\n",
    "        plt.xlabel('Ratio')\n",
    "        plt.ylabel('Density')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"\\nInsufficient nutritional columns for macronutrient ratio analysis.\")\n",
    "\n",
    "\n",
    "    # Energy density analysis\n",
    "    if 'total_weight_g' in df.columns and 'calories_kcal' in df.columns:\n",
    "        df['energy_density'] = df['calories_kcal'] / df['total_weight_g'].replace(0, 1)\n",
    "\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        sns.histplot(df['energy_density'].dropna(), bins=50, kde=True)\n",
    "        plt.title('Distribution of Energy Density (calories/gram)')\n",
    "        plt.xlabel('Energy Density')\n",
    "        plt.ylabel('Count')\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"\\nInsufficient columns for energy density analysis.\")\n",
    "\n",
    "\n",
    "    # 11. Data Quality Assessment\n",
    "    print(\"\\n11. DATA QUALITY ASSESSMENT\")\n",
    "\n",
    "    if 'data_quality_score' in df.columns:\n",
    "        print(f\"Average data quality score: {df['data_quality_score'].mean():.3f}\")\n",
    "        print(f\"Data quality distribution:\")\n",
    "        print(df['data_quality_score'].value_counts().sort_index())\n",
    "\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        sns.histplot(df['data_quality_score'], bins=10, kde=True)\n",
    "        plt.title('Distribution of Data Quality Scores')\n",
    "        plt.xlabel('Data Quality Score')\n",
    "        plt.ylabel('Count')\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"\\n'data_quality_score' column not found for quality assessment.\")\n",
    "\n",
    "\n",
    "    # 12. Outlier Analysis\n",
    "    print(\"\\n12. OUTLIER ANALYSIS\")\n",
    "\n",
    "    def detect_outliers(series, threshold=3):\n",
    "        z_scores = np.abs((series - series.mean()) / series.std())\n",
    "        return z_scores > threshold\n",
    "\n",
    "    outlier_cols = ['calories_kcal', 'protein_g', 'fat_g', 'carbohydrate_g', 'total_weight_g']\n",
    "    outlier_results = {}\n",
    "\n",
    "    for col in outlier_cols:\n",
    "        if col in df.columns:\n",
    "            outliers = detect_outliers(df[col].dropna())\n",
    "            outlier_results[col] = {\n",
    "                'count': outliers.sum(),\n",
    "                'percentage': (outliers.sum() / len(df[col].dropna())) * 100\n",
    "            }\n",
    "        else:\n",
    "            outlier_results[col] = {'count': 0, 'percentage': 0.0}\n",
    "\n",
    "\n",
    "    print(\"Outlier analysis:\")\n",
    "    for col, results in outlier_results.items():\n",
    "        print(f\"  {col}: {results['count']} outliers ({results['percentage']:.2f}%)\")\n",
    "\n",
    "    # 13. Temporal Patterns (Preparation Time)\n",
    "    print(\"\\n13. TEMPORAL PATTERNS\")\n",
    "\n",
    "    if 'estimated_prep_time' in df.columns:\n",
    "        print(f\"Preparation time statistics:\")\n",
    "        print(f\"  Mean: {df['estimated_prep_time'].mean():.2f} minutes\")\n",
    "        print(f\"  Median: {df['estimated_prep_time'].median():.2f} minutes\")\n",
    "        print(f\"  Std: {df['estimated_prep_time'].std():.2f} minutes\")\n",
    "\n",
    "        # Preparation time distribution\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.subplot(1, 2, 1)\n",
    "        sns.histplot(df['estimated_prep_time'].dropna(), bins=30, kde=True)\n",
    "        plt.title('Distribution of Preparation Time')\n",
    "        plt.xlabel('Minutes')\n",
    "\n",
    "        plt.subplot(1, 2, 2)\n",
    "        sns.boxplot(y=df['estimated_prep_time'].dropna())\n",
    "        plt.title('Boxplot of Preparation Time')\n",
    "        plt.ylabel('Minutes')\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        # Preparation time vs complexity\n",
    "        if 'num_ingredients' in df.columns and 'num_cooking_methods' in df.columns:\n",
    "            complexity_corr = df[['estimated_prep_time', 'num_ingredients', 'num_cooking_methods']].corr()\n",
    "            print(\"Correlation between preparation time and complexity:\")\n",
    "            print(complexity_corr['estimated_prep_time'][1:].round(3))\n",
    "        else:\n",
    "             print(\"\\nInsufficient columns for preparation time vs complexity analysis.\")\n",
    "    else:\n",
    "        print(\"\\n'estimated_prep_time' column not found for temporal analysis.\")\n",
    "\n",
    "\n",
    "    # 14. Comprehensive Summary\n",
    "    print(\"\\n14. COMPREHENSIVE SUMMARY\")\n",
    "\n",
    "    summary_stats = {\n",
    "        'total_recipes': len(df),\n",
    "        'avg_ingredients': df['num_ingredients'].mean(),\n",
    "        'avg_cooking_methods': df['num_cooking_methods'].mean(),\n",
    "        'avg_calories': df['calories_kcal'].mean(),\n",
    "        'avg_protein': df['protein_g'].mean(),\n",
    "        'avg_fat': df['fat_g'].mean(),\n",
    "        'avg_carbs': df['carbohydrate_g'].mean(),\n",
    "        'avg_prep_time': df['estimated_prep_time'].mean() if 'estimated_prep_time' in df.columns else None,\n",
    "        'data_quality': df['data_quality_score'].mean() if 'data_quality_score' in df.columns else None\n",
    "    }\n",
    "\n",
    "    print(\"Dataset Summary:\")\n",
    "    for key, value in summary_stats.items():\n",
    "        if value is not None:\n",
    "            if isinstance(value, float):\n",
    "                print(f\"  {key.replace('_', ' ').title()}: {value:.2f}\")\n",
    "            else:\n",
    "                print(f\"  {key.replace('_', ' ').title()}: {value}\")\n",
    "\n",
    "    return {\n",
    "        'ingredient_counts': ingredient_counts,\n",
    "        'method_counts': method_counts,\n",
    "        'nutritional_stats': df[nutrition_cols].describe(),\n",
    "        'outlier_analysis': outlier_results,\n",
    "        'summary_stats': summary_stats\n",
    "    }\n",
    "\n",
    "# Run comprehensive EDA on cleaned data\n",
    "eda_results = comprehensive_eda_cleaned(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BEfcL-jyrXTY"
   },
   "source": [
    "## 3 - Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n--99YxL2zND"
   },
   "source": [
    "### 3.1 Multimodal Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mNR4BBy63Xzk"
   },
   "source": [
    "Extracts multimodal features including:\n",
    "\n",
    "*    Nutritional ratios and distributions\n",
    "\n",
    "*    Ingredient co-occurrence networks\n",
    "\n",
    "*    Cultural cuisine clustering\n",
    "\n",
    "*   Portion size analysis\n",
    "\n",
    "*    Image quality assessment (placeholder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ditHrR802rXI",
    "outputId": "b2786481-81a8-463e-b48b-2eaf606fa453"
   },
   "outputs": [],
   "source": [
    "def multimodal_feature_extraction(df):\n",
    "    \"\"\"Extract multimodal features from the dataset\"\"\"\n",
    "    print(\"=\"*50)\n",
    "    print(\"MULTIMODAL FEATURE EXTRACTION\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "    # 1. Nutritional Analysis Features\n",
    "    print(\"\\n1. Nutritional Analysis Features\")\n",
    "\n",
    "    # Calculate nutritional ratios\n",
    "    df['protein_calorie_ratio'] = df['protein_g'] / df['calories_kcal']\n",
    "    df['fat_calorie_ratio'] = df['fat_g'] / df['calories_kcal']\n",
    "    df['carb_calorie_ratio'] = df['carbohydrate_g'] / df['calories_kcal']\n",
    "\n",
    "    # Replace infinities with NaN\n",
    "    df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "\n",
    "    # 2. Ingredient Network Analysis\n",
    "    print(\"\\n2. Ingredient Network Analysis\")\n",
    "\n",
    "    # Create ingredient co-occurrence matrix\n",
    "    all_ingredients = list(set([ingredient for sublist in df['ingredients_cleaned'].tolist() for ingredient in sublist]))\n",
    "    ingredient_index = {ingredient: idx for idx, ingredient in enumerate(all_ingredients)}\n",
    "\n",
    "    # Initialize co-occurrence matrix\n",
    "    co_occurrence = np.zeros((len(all_ingredients), len(all_ingredients)))\n",
    "\n",
    "    # Fill co-occurrence matrix\n",
    "    for ingredients in df['ingredients_cleaned']:\n",
    "        for i in range(len(ingredients)):\n",
    "            for j in range(i+1, len(ingredients)):\n",
    "                idx1 = ingredient_index[ingredients[i]]\n",
    "                idx2 = ingredient_index[ingredients[j]]\n",
    "                co_occurrence[idx1, idx2] += 1\n",
    "                co_occurrence[idx2, idx1] += 1\n",
    "\n",
    "    # Create ingredient graph\n",
    "    G = nx.Graph()\n",
    "\n",
    "    # Add nodes\n",
    "    for ingredient in all_ingredients:\n",
    "        G.add_node(ingredient)\n",
    "\n",
    "    # Add edges (only for top ingredients to reduce complexity)\n",
    "    top_ingredients = [ingredient for ingredient, count in Counter(all_ingredients).most_common(50)]\n",
    "\n",
    "    for i in range(len(top_ingredients)):\n",
    "        for j in range(i+1, len(top_ingredients)):\n",
    "            ing1, ing2 = top_ingredients[i], top_ingredients[j]\n",
    "            idx1, idx2 = ingredient_index[ing1], ingredient_index[ing2]\n",
    "            weight = co_occurrence[idx1, idx2]\n",
    "            if weight > 0:\n",
    "                G.add_edge(ing1, ing2, weight=weight)\n",
    "\n",
    "    # Calculate network metrics\n",
    "    degree_centrality = nx.degree_centrality(G)\n",
    "    betweenness_centrality = nx.betweenness_centrality(G)\n",
    "\n",
    "    print(f\"Ingredient network created with {G.number_of_nodes()} nodes and {G.number_of_edges()} edges\")\n",
    "    print(\"Top ingredients by degree centrality:\")\n",
    "    for ingredient in sorted(degree_centrality, key=degree_centrality.get, reverse=True)[:10]:\n",
    "        print(f\"{ingredient}: {degree_centrality[ingredient]:.4f}\")\n",
    "\n",
    "    # 3. Cultural Cuisine Clustering\n",
    "    print(\"\\n3. Cultural Cuisine Clustering\")\n",
    "\n",
    "    # Create features for clustering (simplified approach)\n",
    "    # In a real scenario, we'd use more sophisticated text embeddings\n",
    "    cuisine_keywords = {\n",
    "        'italian': ['pasta', 'tomato', 'basil', 'olive oil', 'parmesan'],\n",
    "        'mexican': ['tortilla', 'chili', 'avocado', 'lime', 'cilantro'],\n",
    "        'asian': ['soy sauce', 'ginger', 'garlic', 'rice', 'sesame oil'],\n",
    "        'indian': ['curry', 'turmeric', 'cumin', 'coriander', 'garam masala'],\n",
    "        'american': ['cheese', 'beef', 'potato', 'ketchup', 'mayonnaise']\n",
    "    }\n",
    "\n",
    "    def detect_cuisine(ingredients):\n",
    "        scores = {cuisine: 0 for cuisine in cuisine_keywords}\n",
    "        for ingredient in ingredients:\n",
    "            for cuisine, keywords in cuisine_keywords.items():\n",
    "                if ingredient in keywords:\n",
    "                    scores[cuisine] += 1\n",
    "        if max(scores.values()) == 0:\n",
    "            return 'unknown'\n",
    "        return max(scores.items(), key=lambda x: x[1])[0]\n",
    "\n",
    "    df['predicted_cuisine'] = df['ingredients_cleaned'].apply(detect_cuisine)\n",
    "\n",
    "    print(\"Cuisine distribution:\")\n",
    "    print(df['predicted_cuisine'].value_counts())\n",
    "\n",
    "    # 4. Portion Size Analysis\n",
    "    print(\"\\n4. Portion Size Analysis\")\n",
    "\n",
    "    # Calculate realistic serving sizes based on food type\n",
    "    serving_sizes = df.groupby('food_type_standardized')['total_weight_g'].agg(['mean', 'median', 'std']).round(2)\n",
    "    print(\"Serving sizes by food type:\")\n",
    "    print(serving_sizes)\n",
    "\n",
    "    # Flag unrealistic portion sizes (outside 2 standard deviations)\n",
    "    def flag_unrealistic_portion(row):\n",
    "        if pd.isna(row['total_weight_g']):\n",
    "            return False\n",
    "        food_type = row['food_type_standardized']\n",
    "        # Check if food_type exists in serving_sizes index\n",
    "        if food_type not in serving_sizes.index:\n",
    "            return False # Or handle as appropriate\n",
    "\n",
    "        stats = serving_sizes.loc[food_type]\n",
    "        # Handle potential NaN or Inf in stats\n",
    "        if pd.isna(stats['mean']) or pd.isna(stats['std']):\n",
    "            return False\n",
    "\n",
    "        lower_bound = stats['mean'] - 2 * stats['std']\n",
    "        upper_bound = stats['mean'] + 2 * stats['std']\n",
    "        return row['total_weight_g'] < lower_bound or row['total_weight_g'] > upper_bound\n",
    "\n",
    "    df['unrealistic_portion'] = df.apply(flag_unrealistic_portion, axis=1)\n",
    "    print(f\"Number of unrealistic portion sizes: {df['unrealistic_portion'].sum()}\")\n",
    "\n",
    "    # 5. Image Quality Assessment (simulated - would need actual images)\n",
    "    print(\"\\n5. Image Quality Assessment\")\n",
    "\n",
    "    # This would normally require downloading and processing images\n",
    "    # For demonstration, we'll create placeholder features\n",
    "    df['image_quality_score'] = np.random.uniform(0.7, 1.0, len(df))  # Placeholder\n",
    "\n",
    "    print(\"Image quality score distribution:\")\n",
    "    print(df['image_quality_score'].describe())\n",
    "\n",
    "    return df, G\n",
    "\n",
    "# Extract multimodal features\n",
    "df, ingredient_graph = multimodal_feature_extraction(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V43UsMma23iI"
   },
   "source": [
    "### 3.2 Nutritional Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_PLybMDK3WZI"
   },
   "source": [
    "Engineers advanced nutritional features including:\n",
    "\n",
    "*    Macronutrient ratios\n",
    "\n",
    "*     Health indicators (high-fiber, low-sodium, balanced meal flags)\n",
    "\n",
    "*     Dietary compatibility (vegan, keto, paleo, gluten-free)\n",
    "\n",
    "*     Nutritional density scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "PFnwj2fZDtPe",
    "outputId": "ddd5fd1e-d67d-407f-faea-1c25e5fc74c8"
   },
   "outputs": [],
   "source": [
    "def nutritional_engineering(df):\n",
    "    \"\"\"Create advanced nutritional features\"\"\"\n",
    "    print(\"=\"*50)\n",
    "    print(\"NUTRITIONAL ENGINEERING\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "    # 1. Macronutrient Ratios\n",
    "    print(\"\\n1. Macronutrient Ratios\")\n",
    "\n",
    "    # Calculate macronutrient percentages\n",
    "    total_macros = df['protein_g'] + df['fat_g'] + df['carbohydrate_g']\n",
    "    df['protein_pct'] = df['protein_g'] / total_macros * 100\n",
    "    df['fat_pct'] = df['fat_g'] / total_macros * 100\n",
    "    df['carb_pct'] = df['carbohydrate_g'] / total_macros * 100\n",
    "\n",
    "    # Replace infinities with NaN\n",
    "    df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "\n",
    "    # 2. Health Indicators\n",
    "    print(\"\\n2. Health Indicators\")\n",
    "\n",
    "    # High fiber flag (assuming we had fiber data)\n",
    "    # For demonstration, we'll create a placeholder based on ingredients\n",
    "    high_fiber_ingredients = ['whole wheat', 'oats', 'beans', 'lentils', 'broccoli',\n",
    "                             'avocado', 'berries', 'apples', 'nuts', 'seeds']\n",
    "\n",
    "    def has_high_fiber(ingredients):\n",
    "        return any(ingredient in high_fiber_ingredients for ingredient in ingredients)\n",
    "\n",
    "    df['high_fiber'] = df['ingredients_cleaned'].apply(has_high_fiber)\n",
    "\n",
    "    # Low sodium flag (placeholder)\n",
    "    df['low_sodium'] = np.random.choice([True, False], len(df), p=[0.3, 0.7])\n",
    "\n",
    "    # Balanced meal flag (reasonable distribution of macros)\n",
    "    def is_balanced(row):\n",
    "        if pd.isna(row['protein_pct']) or pd.isna(row['fat_pct']) or pd.isna(row['carb_pct']):\n",
    "            return False\n",
    "        return (15 <= row['protein_pct'] <= 35 and\n",
    "                20 <= row['fat_pct'] <= 40 and\n",
    "                35 <= row['carb_pct'] <= 65)\n",
    "\n",
    "    df['balanced_meal'] = df.apply(is_balanced, axis=1)\n",
    "\n",
    "    print(f\"High fiber meals: {df['high_fiber'].sum()}\")\n",
    "    print(f\"Low sodium meals: {df['low_sodium'].sum()}\")\n",
    "    print(f\"Balanced meals: {df['balanced_meal'].sum()}\")\n",
    "\n",
    "    # 3. Dietary Compatibility\n",
    "    print(\"\\n3. Dietary Compatibility\")\n",
    "\n",
    "    # Define dietary restrictions\n",
    "    vegan_restricted = ['meat', 'chicken', 'beef', 'pork', 'fish', 'seafood', 'egg',\n",
    "                       'dairy', 'milk', 'cheese', 'butter', 'honey', 'gelatin']\n",
    "    keto_restricted = ['sugar', 'honey', 'maple syrup', 'rice', 'pasta', 'bread', 'potato',\n",
    "                      'corn', 'beans', 'grains', 'fruit juice']\n",
    "    paleo_restricted = ['dairy', 'legumes', 'grains', 'processed food', 'refined sugar',\n",
    "                       'vegetable oil', 'soy', 'peanut']\n",
    "    gluten_restricted = ['wheat', 'barley', 'rye', 'bread', 'pasta', 'cereal', 'couscous']\n",
    "\n",
    "    def check_diet_compatibility(ingredients, diet_restrictions):\n",
    "        return not any(ingredient in diet_restrictions for ingredient in ingredients)\n",
    "\n",
    "    df['vegan'] = df['ingredients_cleaned'].apply(lambda x: check_diet_compatibility(x, vegan_restricted))\n",
    "    df['keto'] = df['ingredients_cleaned'].apply(lambda x: check_diet_compatibility(x, keto_restricted))\n",
    "    df['paleo'] = df['ingredients_cleaned'].apply(lambda x: check_diet_compatibility(x, paleo_restricted))\n",
    "    df['gluten_free'] = df['ingredients_cleaned'].apply(lambda x: check_diet_compatibility(x, gluten_restricted))\n",
    "\n",
    "    print(\"Dietary compatibility counts:\")\n",
    "    print(f\"Vegan: {df['vegan'].sum()}\")\n",
    "    print(f\"Keto: {df['keto'].sum()}\")\n",
    "    print(f\"Paleo: {df['paleo'].sum()}\")\n",
    "    print(f\"Gluten-free: {df['gluten_free'].sum()}\")\n",
    "\n",
    "    # 4. Nutritional Density Scores\n",
    "    print(\"\\n4. Nutritional Density Scores\")\n",
    "\n",
    "    # Create a simplified nutritional density score\n",
    "    # In a real scenario, this would incorporate more nutrients\n",
    "    def calculate_nutritional_density(row):\n",
    "        if pd.isna(row['calories_kcal']) or row['calories_kcal'] == 0:\n",
    "            return np.nan\n",
    "\n",
    "        # Base score on protein content (higher protein = more nutrient dense)\n",
    "        protein_score = min(row['protein_g'] / row['calories_kcal'] * 100, 10)\n",
    "\n",
    "        # Adjust based on food type\n",
    "        food_type_bonus = {\n",
    "            'raw_vegetables_fruits': 2,\n",
    "            'homemade': 1,\n",
    "            'restaurant': 0,\n",
    "            'packaged_food': -1\n",
    "        }.get(row['food_type_standardized'], 0)\n",
    "\n",
    "        # Adjust based on balanced meal flag\n",
    "        balance_bonus = 2 if row['balanced_meal'] else 0\n",
    "\n",
    "        return protein_score + food_type_bonus + balance_bonus\n",
    "\n",
    "    df['nutritional_density'] = df.apply(calculate_nutritional_density, axis=1)\n",
    "\n",
    "    print(\"Nutritional density score distribution:\")\n",
    "    print(df['nutritional_density'].describe())\n",
    "\n",
    "    # Visualize nutritional density by food type\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.boxplot(data=df, x='food_type_standardized', y='nutritional_density')\n",
    "    plt.title('Nutritional Density by Food Type')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return df\n",
    "\n",
    "# Perform nutritional engineering\n",
    "df = nutritional_engineering(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oNcjbK2R3v72"
   },
   "source": [
    "## 4 - Machine Learning Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 388
    },
    "id": "Zn1o6iCsXpoU",
    "outputId": "f3de3081-4b2e-4832-b7bd-27b91e5b75f6"
   },
   "outputs": [],
   "source": [
    "def focused_ml_pipeline(df, target_variable='calories_kcal', test_size=0.2, random_state=42):\n",
    "    \"\"\"\n",
    "    Focused ML pipeline with 2-3 models, comparison, and parameter tuning\n",
    "    \"\"\"\n",
    "    print(\"=\"*60)\n",
    "    print(\"FOCUSED MACHINE LEARNING PIPELINE\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    # 1. Data Preparation\n",
    "    print(\"1. PREPARING DATA FOR MACHINE LEARNING...\")\n",
    "\n",
    "    # Select relevant features\n",
    "    feature_columns = [\n",
    "        'protein_g', 'fat_g', 'carbohydrate_g', 'total_weight_g',\n",
    "        'num_ingredients', 'num_cooking_methods', 'cooking_health_score',\n",
    "        'protein_ratio', 'fat_ratio', 'carb_ratio', 'energy_density'\n",
    "    ]\n",
    "\n",
    "    # Filter to only include columns that exist in the dataframe\n",
    "    available_features = [col for col in feature_columns if col in df.columns]\n",
    "\n",
    "    # Handle missing values for selected features\n",
    "    X = df[available_features].copy()\n",
    "    y = df[target_variable]\n",
    "\n",
    "    # Impute missing values\n",
    "    imputer = SimpleImputer(strategy='median')\n",
    "    X_imputed = imputer.fit_transform(X)\n",
    "    X = pd.DataFrame(X_imputed, columns=available_features, index=X.index)\n",
    "\n",
    "    # Remove rows where target is missing\n",
    "    valid_indices = y.notna()\n",
    "    X = X[valid_indices]\n",
    "    y = y[valid_indices]\n",
    "\n",
    "    print(f\"Features used: {available_features}\")\n",
    "    print(f\"Target variable: {target_variable}\")\n",
    "    print(f\"Final dataset shape: {X.shape}\")\n",
    "\n",
    "    # 2. Train-Test Split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=test_size, random_state=random_state\n",
    "    )\n",
    "\n",
    "    # Scale features\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    # 3. Model Selection and Training\n",
    "    print(\"\\n2. TRAINING AND COMPARING MODELS...\")\n",
    "\n",
    "    # Define models to compare\n",
    "    models = {\n",
    "        'Random Forest': RandomForestRegressor(random_state=random_state),\n",
    "        'Gradient Boosting': GradientBoostingRegressor(random_state=random_state),\n",
    "        'XGBoost': XGBRegressor(random_state=random_state, verbosity=0)\n",
    "    }\n",
    "\n",
    "    # Train and evaluate each model\n",
    "    results = {}\n",
    "    for name, model in models.items():\n",
    "        print(f\"Training {name}...\")\n",
    "        model.fit(X_train_scaled, y_train)\n",
    "        y_pred = model.predict(X_test_scaled)\n",
    "\n",
    "        # Calculate metrics\n",
    "        mae = mean_absolute_error(y_test, y_pred)\n",
    "        mse = mean_squared_error(y_test, y_pred)\n",
    "        rmse = np.sqrt(mse)\n",
    "        r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "        results[name] = {\n",
    "            'model': model,\n",
    "            'mae': mae,\n",
    "            'mse': mse,\n",
    "            'rmse': rmse,\n",
    "            'r2': r2,\n",
    "            'predictions': y_pred\n",
    "        }\n",
    "\n",
    "        print(f\"  {name} - R²: {r2:.4f}, MAE: {mae:.2f}, RMSE: {rmse:.2f}\")\n",
    "\n",
    "    # 4. Model Comparison\n",
    "    print(\"\\n3. MODEL COMPARISON\")\n",
    "\n",
    "    # Create comparison table\n",
    "    comparison_df = pd.DataFrame({\n",
    "        'Model': list(results.keys()),\n",
    "        'R² Score': [results[name]['r2'] for name in results],\n",
    "        'MAE': [results[name]['mae'] for name in results],\n",
    "        'RMSE': [results[name]['rmse'] for name in results]\n",
    "    }).sort_values('R² Score', ascending=False)\n",
    "\n",
    "    print(\"Model Performance Comparison:\")\n",
    "    print(comparison_df.to_string(index=False))\n",
    "\n",
    "    # Visual comparison\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "    # R² Score comparison\n",
    "    axes[0].bar(comparison_df['Model'], comparison_df['R² Score'], color=['blue', 'green', 'orange'])\n",
    "    axes[0].set_title('R² Score Comparison')\n",
    "    axes[0].set_ylabel('R² Score')\n",
    "    axes[0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "    # Error metrics comparison\n",
    "    x_pos = np.arange(len(comparison_df))\n",
    "    width = 0.25\n",
    "    axes[1].bar(x_pos - width, comparison_df['MAE'], width, label='MAE', color='red')\n",
    "    axes[1].bar(x_pos, comparison_df['RMSE'], width, label='RMSE', color='purple')\n",
    "    axes[1].set_title('Error Metrics Comparison')\n",
    "    axes[1].set_ylabel('Error')\n",
    "    axes[1].set_xticks(x_pos)\n",
    "    axes[1].set_xticklabels(comparison_df['Model'], rotation=45)\n",
    "    axes[1].legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # 5. Residual Analysis\n",
    "    print(\"\\n4. RESIDUAL ANALYSIS\")\n",
    "\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "    for i, (name, result) in enumerate(results.items()):\n",
    "        residuals = y_test - result['predictions']\n",
    "\n",
    "        axes[i].scatter(result['predictions'], residuals, alpha=0.6)\n",
    "        axes[i].axhline(y=0, color='red', linestyle='--')\n",
    "        axes[i].set_xlabel('Predicted Values')\n",
    "        axes[i].set_ylabel('Residuals')\n",
    "        axes[i].set_title(f'{name} - Residual Plot')\n",
    "\n",
    "        # Add residual statistics\n",
    "        residual_mean = residuals.mean()\n",
    "        residual_std = residuals.std()\n",
    "        axes[i].text(0.05, 0.95, f'Mean: {residual_mean:.2f}\\nStd: {residual_std:.2f}',\n",
    "                    transform=axes[i].transAxes, verticalalignment='top')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # 6. Feature Importance Analysis\n",
    "    print(\"\\n5. FEATURE IMPORTANCE ANALYSIS\")\n",
    "\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 8))\n",
    "\n",
    "    for i, (name, result) in enumerate(results.items()):\n",
    "        if hasattr(result['model'], 'feature_importances_'):\n",
    "            importance = result['model'].feature_importances_\n",
    "            feature_importance = pd.DataFrame({\n",
    "                'feature': available_features,\n",
    "                'importance': importance\n",
    "            }).sort_values('importance', ascending=True)\n",
    "\n",
    "            axes[i].barh(feature_importance['feature'], feature_importance['importance'])\n",
    "            axes[i].set_title(f'{name} - Feature Importance')\n",
    "            axes[i].set_xlabel('Importance')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # 7. Hyperparameter Tuning for Best Model\n",
    "    print(\"\\n6. HYPERPARAMETER TUNING\")\n",
    "\n",
    "    # Select best model based on R² score\n",
    "    best_model_name = comparison_df.iloc[0]['Model']\n",
    "    best_model = results[best_model_name]['model']\n",
    "\n",
    "    print(f\"Tuning hyperparameters for {best_model_name}...\")\n",
    "\n",
    "    # Define parameter grids for each model type\n",
    "    param_grids = {\n",
    "        'Random Forest': {\n",
    "            'n_estimators': [50, 100, 200],\n",
    "            'max_depth': [None, 10, 20, 30],\n",
    "            'min_samples_split': [2, 5, 10],\n",
    "            'min_samples_leaf': [1, 2, 4]\n",
    "        },\n",
    "        'Gradient Boosting': {\n",
    "            'n_estimators': [50, 100, 200],\n",
    "            'learning_rate': [0.01, 0.05, 0.1],\n",
    "            'max_depth': [3, 5, 7],\n",
    "            'subsample': [0.8, 0.9, 1.0]\n",
    "        },\n",
    "        'XGBoost': {\n",
    "            'n_estimators': [50, 100, 200],\n",
    "            'learning_rate': [0.01, 0.05, 0.1],\n",
    "            'max_depth': [3, 5, 7],\n",
    "            'subsample': [0.8, 0.9, 1.0],\n",
    "            'colsample_bytree': [0.8, 0.9, 1.0]\n",
    "        }\n",
    "    }\n",
    "\n",
    "    # Perform grid search with cross-validation\n",
    "    if best_model_name in param_grids:\n",
    "        grid_search = GridSearchCV(\n",
    "            estimator=type(best_model)(random_state=random_state),\n",
    "            param_grid=param_grids[best_model_name],\n",
    "            cv=5,\n",
    "            scoring='r2',\n",
    "            n_jobs=-1,\n",
    "            verbose=1\n",
    "        )\n",
    "\n",
    "        grid_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "        print(f\"Best parameters for {best_model_name}:\")\n",
    "        for param, value in grid_search.best_params_.items():\n",
    "            print(f\"  {param}: {value}\")\n",
    "\n",
    "        # Evaluate tuned model\n",
    "        tuned_model = grid_search.best_estimator_\n",
    "        y_pred_tuned = tuned_model.predict(X_test_scaled)\n",
    "\n",
    "        tuned_r2 = r2_score(y_test, y_pred_tuned)\n",
    "        tuned_mae = mean_absolute_error(y_test, y_pred_tuned)\n",
    "\n",
    "        print(f\"Tuned {best_model_name} Performance:\")\n",
    "        print(f\"  R²: {tuned_r2:.4f} (Before: {results[best_model_name]['r2']:.4f})\")\n",
    "        print(f\"  MAE: {tuned_mae:.2f} (Before: {results[best_model_name]['mae']:.2f})\")\n",
    "\n",
    "        # Update results with tuned model\n",
    "        results[f'Tuned {best_model_name}'] = {\n",
    "            'model': tuned_model,\n",
    "            'mae': tuned_mae,\n",
    "            'r2': tuned_r2,\n",
    "            'predictions': y_pred_tuned\n",
    "        }\n",
    "\n",
    "    # 8. Final Model Evaluation\n",
    "    print(\"\\n7. FINAL MODEL EVALUATION\")\n",
    "\n",
    "    # Compare original vs tuned model\n",
    "    if f'Tuned {best_model_name}' in results:\n",
    "        improvement_r2 = results[f'Tuned {best_model_name}']['r2'] - results[best_model_name]['r2']\n",
    "        improvement_mae = results[best_model_name]['mae'] - results[f'Tuned {best_model_name}']['mae']\n",
    "\n",
    "        print(f\"Improvement after tuning:\")\n",
    "        print(f\"  R²: +{improvement_r2:.4f}\")\n",
    "        print(f\"  MAE: -{improvement_mae:.2f}\")\n",
    "\n",
    "    # 9. Prediction Visualization\n",
    "    print(\"\\n8. PREDICTION VISUALIZATION\")\n",
    "\n",
    "    plt.figure(figsize=(12, 10))\n",
    "\n",
    "    # Plot actual vs predicted for best model\n",
    "    best_predictions = results[best_model_name]['predictions']\n",
    "\n",
    "    plt.subplot(2, 2, 1)\n",
    "    plt.scatter(y_test, best_predictions, alpha=0.6)\n",
    "    plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)\n",
    "    plt.xlabel('Actual Values')\n",
    "    plt.ylabel('Predicted Values')\n",
    "    plt.title(f'{best_model_name} - Actual vs Predicted')\n",
    "\n",
    "    # Error distribution\n",
    "    plt.subplot(2, 2, 2)\n",
    "    errors = y_test - best_predictions\n",
    "    sns.histplot(errors, kde=True)\n",
    "    plt.xlabel('Prediction Error')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title('Error Distribution')\n",
    "\n",
    "    # Feature importance for best model\n",
    "    plt.subplot(2, 2, 3)\n",
    "    if hasattr(results[best_model_name]['model'], 'feature_importances_'):\n",
    "        importance = results[best_model_name]['model'].feature_importances_\n",
    "        feature_importance = pd.DataFrame({\n",
    "            'feature': available_features,\n",
    "            'importance': importance\n",
    "        }).sort_values('importance', ascending=True)\n",
    "\n",
    "        plt.barh(feature_importance['feature'], feature_importance['importance'])\n",
    "        plt.xlabel('Importance')\n",
    "        plt.title('Feature Importance')\n",
    "\n",
    "    # Model comparison bar chart\n",
    "    plt.subplot(2, 2, 4)\n",
    "    model_names = []\n",
    "    r2_scores = []\n",
    "\n",
    "    for name, result in results.items():\n",
    "        if 'r2' in result:\n",
    "            model_names.append(name)\n",
    "            r2_scores.append(result['r2'])\n",
    "\n",
    "    plt.bar(model_names, r2_scores, color=['blue', 'green', 'orange', 'red'])\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.ylabel('R² Score')\n",
    "    plt.title('Model Comparison (R² Score)')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # 10. Model Interpretation (SHAP values for best model)\n",
    "    print(\"\\n9. MODEL INTERPRETATION (SHAP VALUES)\")\n",
    "\n",
    "    try:\n",
    "        import shap\n",
    "\n",
    "        # Use the best model for interpretation\n",
    "        explainer = shap.TreeExplainer(results[best_model_name]['model'])\n",
    "        shap_values = explainer.shap_values(X_test_scaled)\n",
    "\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        shap.summary_plot(shap_values, X_test_scaled, feature_names=available_features, show=False)\n",
    "        plt.title(f'SHAP Summary Plot - {best_model_name}')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        # Force plot for a specific instance\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        shap.force_plot(explainer.expected_value, shap_values[0], X_test_scaled[0],\n",
    "                       feature_names=available_features, show=False, matplotlib=True)\n",
    "        plt.title(f'SHAP Force Plot - First Instance')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    except ImportError:\n",
    "        print(\"SHAP not installed. Install with: pip install shap\")\n",
    "\n",
    "    # 11. Final Recommendations\n",
    "    print(\"\\n10. FINAL RECOMMENDATIONS\")\n",
    "\n",
    "    best_overall = max(results.items(), key=lambda x: x[1]['r2'] if 'r2' in x[1] else -1)\n",
    "    print(f\"Best performing model: {best_overall[0]}\")\n",
    "    print(f\"Best R² Score: {best_overall[1]['r2']:.4f}\")\n",
    "    print(f\"Best MAE: {best_overall[1]['mae']:.2f}\")\n",
    "\n",
    "    # Key insights\n",
    "    print(\"\\nKey Insights:\")\n",
    "    print(\"- Feature importance reveals which factors most influence calorie content\")\n",
    "    print(\"- Residual analysis shows model bias and error patterns\")\n",
    "    print(\"- Hyperparameter tuning can significantly improve model performance\")\n",
    "\n",
    "    return {\n",
    "        'results': results,\n",
    "        'best_model': best_overall[0],\n",
    "        'best_score': best_overall[1]['r2'],\n",
    "        'feature_importance': feature_importance if 'feature_importance' in locals() else None,\n",
    "        'X_test': X_test,\n",
    "        'y_test': y_test,\n",
    "        'scaler': scaler,\n",
    "        'imputer': imputer\n",
    "    }\n",
    "\n",
    "# Run the focused ML pipeline\n",
    "ml_results = focused_ml_pipeline(df, target_variable='calories_kcal')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Yv7DJEnN9hfE"
   },
   "source": [
    "## 5 - ML advanced"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hH7OBLoMVEnS"
   },
   "source": [
    "### 5.1 recommendations systems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IZVrrjDsVuYM"
   },
   "source": [
    "This comprehensive recommendation system provides:\n",
    "\n",
    "  - Diet-Specific Recommendations: Vegan, vegetarian, keto, paleo, gluten-free\n",
    "\n",
    "  - Ingredient-Based Filtering: Preferred and excluded ingredients\n",
    "\n",
    "  - Cooking Method Preferences: Preferred and excluded cooking methods\n",
    "\n",
    "  - Health Goal Optimization: Weight loss, muscle gain, maintenance, etc.\n",
    "\n",
    "  - Similarity-Based Recommendations: Recipes similar to liked dishes\n",
    "\n",
    "  - User Profiles: Personalized recommendations based on comprehensive profiles\n",
    "\n",
    "  - Advanced Filtering: Calorie ranges, prep time, nutritional density"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LKpomz9TVB_R"
   },
   "outputs": [],
   "source": [
    "class NutritionalRecommender:\n",
    "    \"\"\"\n",
    "    Advanced recommendation system for recipes based on multiple criteria:\n",
    "    - Diet type (vegan, keto, paleo, gluten-free, etc.)\n",
    "    - Ingredient preferences/allergies\n",
    "    - Cooking method preferences\n",
    "    - Health preferences (calorie density, nutritional density, etc.)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, df):\n",
    "        self.df = df.copy()\n",
    "        self._prepare_data()\n",
    "        self._build_feature_vectors()\n",
    "\n",
    "    def _prepare_data(self):\n",
    "        \"\"\"Prepare data for recommendation system\"\"\"\n",
    "        # Ensure we have the necessary columns\n",
    "        required_columns = ['ingredients_cleaned', 'cooking_methods_cleaned',\n",
    "                           'calories_kcal', 'protein_g', 'fat_g', 'carbohydrate_g',\n",
    "                           'nutritional_density', 'total_weight_g']\n",
    "\n",
    "        for col in required_columns:\n",
    "            if col not in self.df.columns:\n",
    "                raise ValueError(f\"Missing required column: {col}\")\n",
    "\n",
    "        # Calculate additional metrics\n",
    "        self.df['energy_density'] = self.df['calories_kcal'] / self.df['total_weight_g'].replace(0, 1)\n",
    "        self.df['protein_density'] = self.df['protein_g'] / self.df['total_weight_g'].replace(0, 1)\n",
    "\n",
    "        # Create diet type flags if not already present\n",
    "        if 'vegan' not in self.df.columns:\n",
    "            vegan_restricted = ['meat', 'chicken', 'beef', 'pork', 'fish', 'seafood', 'egg',\n",
    "                               'dairy', 'milk', 'cheese', 'butter', 'honey', 'gelatin']\n",
    "            self.df['vegan'] = self.df['ingredients_cleaned'].apply(\n",
    "                lambda x: not any(any(restr in ing for restr in vegan_restricted) for ing in x)\n",
    "            )\n",
    "\n",
    "        if 'vegetarian' not in self.df.columns:\n",
    "            vegetarian_restricted = ['meat', 'chicken', 'beef', 'pork', 'fish', 'seafood']\n",
    "            self.df['vegetarian'] = self.df['ingredients_cleaned'].apply(\n",
    "                lambda x: not any(any(restr in ing for restr in vegetarian_restricted) for ing in x)\n",
    "            )\n",
    "\n",
    "    def _build_feature_vectors(self):\n",
    "        \"\"\"Build feature vectors for similarity calculations\"\"\"\n",
    "        # Create ingredient-based features\n",
    "        all_ingredients = list(set([ing for sublist in self.df['ingredients_cleaned'] for ing in sublist]))\n",
    "        self.ingredient_vectorizer = {ing: idx for idx, ing in enumerate(all_ingredients)}\n",
    "\n",
    "        # Create cooking method features\n",
    "        all_methods = list(set([method for sublist in self.df['cooking_methods_cleaned'] for method in sublist]))\n",
    "        self.method_vectorizer = {method: idx for idx, method in enumerate(all_methods)}\n",
    "\n",
    "        # Build feature matrix\n",
    "        self.feature_vectors = []\n",
    "        for _, row in self.df.iterrows():\n",
    "            # Ingredient features (one-hot encoded)\n",
    "            ing_vector = np.zeros(len(self.ingredient_vectorizer))\n",
    "            for ing in row['ingredients_cleaned']:\n",
    "                if ing in self.ingredient_vectorizer:\n",
    "                    ing_vector[self.ingredient_vectorizer[ing]] = 1\n",
    "\n",
    "            # Cooking method features\n",
    "            method_vector = np.zeros(len(self.method_vectorizer))\n",
    "            for method in row['cooking_methods_cleaned']:\n",
    "                if method in self.method_vectorizer:\n",
    "                    method_vector[self.method_vectorizer[method]] = 1\n",
    "\n",
    "            # Nutritional features (normalized)\n",
    "            nutrition_vector = np.array([\n",
    "                row['calories_kcal'] / 1000,  # Scale to reasonable range\n",
    "                row['protein_g'] / 100,\n",
    "                row['fat_g'] / 100,\n",
    "                row['carbohydrate_g'] / 100,\n",
    "                row['nutritional_density'] / 10,\n",
    "                row['energy_density']\n",
    "            ])\n",
    "\n",
    "            # Combine all features\n",
    "            combined_vector = np.concatenate([ing_vector, method_vector, nutrition_vector])\n",
    "            self.feature_vectors.append(combined_vector)\n",
    "\n",
    "        self.feature_vectors = np.array(self.feature_vectors)\n",
    "        self.scaler = StandardScaler()\n",
    "        self.scaled_vectors = self.scaler.fit_transform(self.feature_vectors)\n",
    "\n",
    "    def recommend_by_diet(self, diet_type, n_recommendations=5, **filters):\n",
    "        \"\"\"\n",
    "        Recommend recipes based on specific diet type\n",
    "        Supported diets: vegan, vegetarian, keto, paleo, gluten_free\n",
    "        \"\"\"\n",
    "        valid_diets = ['vegan', 'vegetarian', 'keto', 'paleo', 'gluten_free']\n",
    "        if diet_type not in valid_diets:\n",
    "            raise ValueError(f\"Diet type must be one of: {valid_diets}\")\n",
    "\n",
    "        if diet_type not in self.df.columns:\n",
    "            raise ValueError(f\"Diet information for {diet_type} not available in dataset\")\n",
    "\n",
    "        # Filter by diet\n",
    "        diet_recipes = self.df[self.df[diet_type] == True]\n",
    "\n",
    "        if len(diet_recipes) == 0:\n",
    "            return self._fallback_recommendations(f\"No {diet_type} recipes found\", n_recommendations)\n",
    "\n",
    "        # Apply additional filters\n",
    "        filtered_recipes = self._apply_filters(diet_recipes, **filters)\n",
    "\n",
    "        if len(filtered_recipes) == 0:\n",
    "            return self._fallback_recommendations(f\"No {diet_type} recipes match your filters\", n_recommendations)\n",
    "\n",
    "        # Sort by nutritional density or other criteria\n",
    "        recommendations = filtered_recipes.nlargest(n_recommendations, 'nutritional_density')\n",
    "\n",
    "        return self._format_recommendations(recommendations, f\"{diet_type.title()} Recommendations\")\n",
    "\n",
    "    def recommend_by_ingredients(self, preferred_ingredients=[], excluded_ingredients=[],\n",
    "                               n_recommendations=5, **filters):\n",
    "        \"\"\"\n",
    "        Recommend recipes based on ingredient preferences and exclusions\n",
    "        \"\"\"\n",
    "        # Filter recipes\n",
    "        filtered_recipes = self.df.copy()\n",
    "\n",
    "        # Include preferred ingredients\n",
    "        if preferred_ingredients:\n",
    "            preferred_ingredients = [ing.lower() for ing in preferred_ingredients]\n",
    "            filtered_recipes = filtered_recipes[\n",
    "                filtered_recipes['ingredients_cleaned'].apply(\n",
    "                    lambda x: any(pref_ing in ing for ing in x for pref_ing in preferred_ingredients)\n",
    "                )\n",
    "            ]\n",
    "\n",
    "        # Exclude unwanted ingredients\n",
    "        if excluded_ingredients:\n",
    "            excluded_ingredients = [ing.lower() for ing in excluded_ingredients]\n",
    "            filtered_recipes = filtered_recipes[\n",
    "                filtered_recipes['ingredients_cleaned'].apply(\n",
    "                    lambda x: not any(excl_ing in ing for ing in x for excl_ing in excluded_ingredients)\n",
    "                )\n",
    "            ]\n",
    "\n",
    "        # Apply additional filters\n",
    "        filtered_recipes = self._apply_filters(filtered_recipes, **filters)\n",
    "\n",
    "        if len(filtered_recipes) == 0:\n",
    "            return self._fallback_recommendations(\"No recipes match your ingredient preferences\", n_recommendations)\n",
    "\n",
    "        # Score by ingredient match\n",
    "        def ingredient_match_score(recipe_ingredients):\n",
    "            score = 0\n",
    "            for pref_ing in preferred_ingredients:\n",
    "                if any(pref_ing in ing for ing in recipe_ingredients):\n",
    "                    score += 2  # Bonus for preferred ingredients\n",
    "            return score\n",
    "\n",
    "        filtered_recipes['ingredient_score'] = filtered_recipes['ingredients_cleaned'].apply(ingredient_match_score)\n",
    "\n",
    "        # Sort by ingredient score and nutritional density\n",
    "        recommendations = filtered_recipes.sort_values(\n",
    "            ['ingredient_score', 'nutritional_density'], ascending=[False, False]\n",
    "        ).head(n_recommendations)\n",
    "\n",
    "        return self._format_recommendations(recommendations, \"Ingredient-based Recommendations\")\n",
    "\n",
    "    def recommend_by_cooking_method(self, preferred_methods=[], excluded_methods=[],\n",
    "                                  n_recommendations=5, **filters):\n",
    "        \"\"\"\n",
    "        Recommend recipes based on cooking method preferences\n",
    "        \"\"\"\n",
    "        filtered_recipes = self.df.copy()\n",
    "\n",
    "        # Include preferred methods\n",
    "        if preferred_methods:\n",
    "            preferred_methods = [method.lower() for method in preferred_methods]\n",
    "            filtered_recipes = filtered_recipes[\n",
    "                filtered_recipes['cooking_methods_cleaned'].apply(\n",
    "                    lambda x: any(pref_method in method for method in x for pref_method in preferred_methods)\n",
    "                )\n",
    "            ]\n",
    "\n",
    "        # Exclude unwanted methods\n",
    "        if excluded_methods:\n",
    "            excluded_methods = [method.lower() for method in excluded_methods]\n",
    "            filtered_recipes = filtered_recipes[\n",
    "                filtered_recipes['cooking_methods_cleaned'].apply(\n",
    "                    lambda x: not any(excl_method in method for method in x for excl_method in excluded_methods)\n",
    "                )\n",
    "            ]\n",
    "\n",
    "        # Apply additional filters\n",
    "        filtered_recipes = self._apply_filters(filtered_recipes, **filters)\n",
    "\n",
    "        if len(filtered_recipes) == 0:\n",
    "            return self._fallback_recommendations(\"No recipes match your cooking method preferences\", n_recommendations)\n",
    "\n",
    "        # Score by method match\n",
    "        def method_match_score(recipe_methods):\n",
    "            score = 0\n",
    "            for pref_method in preferred_methods:\n",
    "                if any(pref_method in method for method in recipe_methods):\n",
    "                    score += 1\n",
    "            return score\n",
    "\n",
    "        filtered_recipes['method_score'] = filtered_recipes['cooking_methods_cleaned'].apply(method_match_score)\n",
    "\n",
    "        # Sort by method score and nutritional density\n",
    "        recommendations = filtered_recipes.sort_values(\n",
    "            ['method_score', 'nutritional_density'], ascending=[False, False]\n",
    "        ).head(n_recommendations)\n",
    "\n",
    "        return self._format_recommendations(recommendations, \"Cooking Method Recommendations\")\n",
    "\n",
    "    def recommend_by_health_goals(self, goal_type, target_calories=None, n_recommendations=5, **filters):\n",
    "        \"\"\"\n",
    "        Recommend recipes based on health goals\n",
    "        Supported goals: weight_loss, muscle_gain, maintenance, high_protein, low_carb, low_fat\n",
    "        \"\"\"\n",
    "        goal_strategies = {\n",
    "            'weight_loss': {\n",
    "                'max_calories': 400,\n",
    "                'min_protein': 15,\n",
    "                'max_carbs': 40,\n",
    "                'sort_by': ['energy_density', 'nutritional_density'],\n",
    "                'ascending': [True, False]\n",
    "            },\n",
    "            'muscle_gain': {\n",
    "                'min_protein': 25,\n",
    "                'min_calories': 400,\n",
    "                'sort_by': ['protein_density', 'calories_kcal'],\n",
    "                'ascending': [False, False]\n",
    "            },\n",
    "            'maintenance': {\n",
    "                'min_nutritional_density': 5,\n",
    "                'sort_by': ['nutritional_density', 'cooking_health_score'],\n",
    "                'ascending': [False, False]\n",
    "            },\n",
    "            'high_protein': {\n",
    "                'min_protein': 20,\n",
    "                'min_protein_ratio': 0.3,\n",
    "                'sort_by': ['protein_g', 'protein_density'],\n",
    "                'ascending': [False, False]\n",
    "            },\n",
    "            'low_carb': {\n",
    "                'max_carbs': 20,\n",
    "                'sort_by': ['carbohydrate_g', 'nutritional_density'],\n",
    "                'ascending': [True, False]\n",
    "            },\n",
    "            'low_fat': {\n",
    "                'max_fat': 10,\n",
    "                'sort_by': ['fat_g', 'nutritional_density'],\n",
    "                'ascending': [True, False]\n",
    "            }\n",
    "        }\n",
    "\n",
    "        if goal_type not in goal_strategies:\n",
    "            raise ValueError(f\"Goal type must be one of: {list(goal_strategies.keys())}\")\n",
    "\n",
    "        strategy = goal_strategies[goal_type]\n",
    "        filtered_recipes = self.df.copy()\n",
    "\n",
    "        # Apply goal-specific filters\n",
    "        for key, value in strategy.items():\n",
    "            if key.startswith('min_'):\n",
    "                col = key[4:]\n",
    "                if col in filtered_recipes.columns:\n",
    "                    filtered_recipes = filtered_recipes[filtered_recipes[col] >= value]\n",
    "            elif key.startswith('max_'):\n",
    "                col = key[4:]\n",
    "                if col in filtered_recipes.columns:\n",
    "                    filtered_recipes = filtered_recipes[filtered_recipes[col] <= value]\n",
    "\n",
    "        # Apply target calories if specified\n",
    "        if target_calories:\n",
    "            filtered_recipes = filtered_recipes[\n",
    "                filtered_recipes['calories_kcal'].between(\n",
    "                    target_calories * 0.8, target_calories * 1.2\n",
    "                )\n",
    "            ]\n",
    "\n",
    "        # Apply additional filters\n",
    "        filtered_recipes = self._apply_filters(filtered_recipes, **filters)\n",
    "\n",
    "        if len(filtered_recipes) == 0:\n",
    "            return self._fallback_recommendations(f\"No recipes match your {goal_type} goals\", n_recommendations)\n",
    "\n",
    "        # Sort according to strategy\n",
    "        recommendations = filtered_recipes.sort_values(\n",
    "            strategy['sort_by'], ascending=strategy['ascending']\n",
    "        ).head(n_recommendations)\n",
    "\n",
    "        return self._format_recommendations(recommendations, f\"{goal_type.replace('_', ' ').title()} Recommendations\")\n",
    "\n",
    "    def recommend_similar_to_recipe(self, recipe_id, n_recommendations=5, **filters):\n",
    "        \"\"\"\n",
    "        Recommend recipes similar to a given recipe\n",
    "        \"\"\"\n",
    "        if recipe_id >= len(self.df) or recipe_id < 0:\n",
    "            raise ValueError(\"Invalid recipe ID\")\n",
    "\n",
    "        # Get target recipe features\n",
    "        target_vector = self.scaled_vectors[recipe_id]\n",
    "\n",
    "        # Calculate similarities\n",
    "        similarities = cosine_similarity([target_vector], self.scaled_vectors)[0]\n",
    "\n",
    "        # Apply filters\n",
    "        filtered_indices = self._get_filtered_indices(**filters)\n",
    "\n",
    "        if len(filtered_indices) == 0:\n",
    "            return self._fallback_recommendations(\"No similar recipes match your filters\", n_recommendations)\n",
    "\n",
    "        # Get similar recipes (excluding the target itself)\n",
    "        similar_indices = []\n",
    "        for idx in np.argsort(similarities)[::-1]:\n",
    "            if idx != recipe_id and idx in filtered_indices:\n",
    "                similar_indices.append(idx)\n",
    "            if len(similar_indices) >= n_recommendations:\n",
    "                break\n",
    "\n",
    "        if not similar_indices:\n",
    "            return self._fallback_recommendations(\"No similar recipes found\", n_recommendations)\n",
    "\n",
    "        recommendations = self.df.iloc[similar_indices].copy()\n",
    "        recommendations['similarity_score'] = similarities[similar_indices]\n",
    "\n",
    "        return self._format_recommendations(recommendations, \"Similar Recipes\")\n",
    "\n",
    "    def _apply_filters(self, recipes, **filters):\n",
    "        \"\"\"Apply additional filters to recipes\"\"\"\n",
    "        filtered = recipes.copy()\n",
    "\n",
    "        # Calorie range\n",
    "        if 'min_calories' in filters:\n",
    "            filtered = filtered[filtered['calories_kcal'] >= filters['min_calories']]\n",
    "        if 'max_calories' in filters:\n",
    "            filtered = filtered[filtered['calories_kcal'] <= filters['max_calories']]\n",
    "\n",
    "        # Protein range\n",
    "        if 'min_protein' in filters:\n",
    "            filtered = filtered[filtered['protein_g'] >= filters['min_protein']]\n",
    "        if 'max_protein' in filters:\n",
    "            filtered = filtered[filtered['protein_g'] <= filters['max_protein']]\n",
    "\n",
    "        # Preparation time\n",
    "        if 'max_prep_time' in filters and 'estimated_prep_time' in filtered.columns:\n",
    "            filtered = filtered[filtered['estimated_prep_time'] <= filters['max_prep_time']]\n",
    "\n",
    "        # Nutritional density\n",
    "        if 'min_nutritional_density' in filters:\n",
    "            filtered = filtered[filtered['nutritional_density'] >= filters['min_nutritional_density']]\n",
    "\n",
    "        # Food type\n",
    "        if 'food_type' in filters and 'food_type_standardized' in filtered.columns:\n",
    "            filtered = filtered[filtered['food_type_standardized'] == filters['food_type']]\n",
    "\n",
    "        return filtered\n",
    "\n",
    "    def _get_filtered_indices(self, **filters):\n",
    "        \"\"\"Get indices of recipes that match the filters\"\"\"\n",
    "        filtered = self._apply_filters(self.df, **filters)\n",
    "        return filtered.index.tolist()\n",
    "\n",
    "    def _fallback_recommendations(self, message, n_recommendations):\n",
    "        \"\"\"Provide fallback recommendations when no matches are found\"\"\"\n",
    "        print(f\"Warning: {message}. Showing general recommendations instead.\")\n",
    "\n",
    "        # Fall back to general healthy recommendations\n",
    "        fallback = self.df.nlargest(n_recommendations, 'nutritional_density')\n",
    "        return self._format_recommendations(fallback, \"General Healthy Recommendations (Fallback)\")\n",
    "\n",
    "    def _format_recommendations(self, recommendations, title):\n",
    "        \"\"\"Format recommendations for display\"\"\"\n",
    "        result = {\n",
    "            'title': title,\n",
    "            'count': len(recommendations),\n",
    "            'recipes': []\n",
    "        }\n",
    "\n",
    "        for _, recipe in recommendations.iterrows():\n",
    "            recipe_info = {\n",
    "                'id': recipe.name if hasattr(recipe, 'name') else None,\n",
    "                'dish_name': recipe.get('dish_name_cleaned', recipe.get('dish_name', 'Unknown')),\n",
    "                'calories': round(recipe['calories_kcal'], 1),\n",
    "                'protein': round(recipe['protein_g'], 1),\n",
    "                'carbs': round(recipe['carbohydrate_g'], 1),\n",
    "                'fat': round(recipe['fat_g'], 1),\n",
    "                'nutritional_density': round(recipe.get('nutritional_density', 0), 2),\n",
    "                'ingredients': recipe['ingredients_cleaned'][:5],  # Top 5 ingredients\n",
    "                'cooking_methods': recipe['cooking_methods_cleaned'],\n",
    "                'prep_time': round(recipe.get('estimated_prep_time', 0)) if 'estimated_prep_time' in recipe else 'Unknown'\n",
    "            }\n",
    "\n",
    "            # Add similarity score if available\n",
    "            if 'similarity_score' in recipe:\n",
    "                recipe_info['similarity'] = round(recipe['similarity_score'], 3)\n",
    "\n",
    "            result['recipes'].append(recipe_info)\n",
    "\n",
    "        return result\n",
    "\n",
    "    def print_recommendations(self, recommendations):\n",
    "        \"\"\"Print recommendations in a user-friendly format\"\"\"\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"{recommendations['title']}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        print(f\"Found {recommendations['count']} recipes\\n\")\n",
    "\n",
    "        for i, recipe in enumerate(recommendations['recipes'], 1):\n",
    "            print(f\"{i}. {recipe['dish_name']}\")\n",
    "            print(f\"   Calories: {recipe['calories']} kcal\")\n",
    "            print(f\"   Protein: {recipe['protein']}g | Carbs: {recipe['carbs']}g | Fat: {recipe['fat']}g\")\n",
    "            print(f\"   Nutritional Density: {recipe['nutritional_density']}\")\n",
    "            print(f\"   Prep Time: {recipe['prep_time']} minutes\")\n",
    "            print(f\"   Cooking Methods: {', '.join(recipe['cooking_methods'])}\")\n",
    "            print(f\"   Key Ingredients: {', '.join(recipe['ingredients'])}\")\n",
    "            if 'similarity' in recipe:\n",
    "                print(f\"   Similarity Score: {recipe['similarity']}\")\n",
    "            print()\n",
    "\n",
    "# Example usage and demonstration\n",
    "def demonstrate_recommendation_system(df):\n",
    "    \"\"\"Demonstrate the recommendation system with various examples\"\"\"\n",
    "    print(\"Initializing Recommendation System...\")\n",
    "    recommender = NutritionalRecommender(df)\n",
    "\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"RECOMMENDATION SYSTEM DEMONSTRATION\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    # 1. Diet-based recommendations\n",
    "    print(\"\\n1. VEGAN RECOMMENDATIONS\")\n",
    "    vegan_recs = recommender.recommend_by_diet('vegan', n_recommendations=3, max_calories=500)\n",
    "    recommender.print_recommendations(vegan_recs)\n",
    "\n",
    "    # 2. Ingredient-based recommendations\n",
    "    print(\"\\n2. CHICKEN AND VEGETABLE RECIPES\")\n",
    "    ingredient_recs = recommender.recommend_by_ingredients(\n",
    "        preferred_ingredients=['chicken', 'vegetable', 'broccoli'],\n",
    "        excluded_ingredients=['cheese', 'cream'],\n",
    "        n_recommendations=3,\n",
    "        min_protein=20\n",
    "    )\n",
    "    recommender.print_recommendations(ingredient_recs)\n",
    "\n",
    "    # 3. Cooking method recommendations\n",
    "    print(\"\\n3. GRILLED OR BAKED RECIPES\")\n",
    "    cooking_recs = recommender.recommend_by_cooking_method(\n",
    "        preferred_methods=['grill', 'bake', 'roast'],\n",
    "        excluded_methods=['fry', 'deep fry'],\n",
    "        n_recommendations=3,\n",
    "        max_calories=600\n",
    "    )\n",
    "    recommender.print_recommendations(cooking_recs)\n",
    "\n",
    "    # 4. Health goal recommendations\n",
    "    print(\"\\n4. WEIGHT LOSS RECIPES\")\n",
    "    health_recs = recommender.recommend_by_health_goals(\n",
    "        'weight_loss',\n",
    "        n_recommendations=3,\n",
    "        max_prep_time=30\n",
    "    )\n",
    "    recommender.print_recommendations(health_recs)\n",
    "\n",
    "    print(\"\\n5. HIGH PROTEIN RECIPES FOR MUSCLE GAIN\")\n",
    "    muscle_recs = recommender.recommend_by_health_goals(\n",
    "        'muscle_gain',\n",
    "        n_recommendations=3,\n",
    "        min_protein=30\n",
    "    )\n",
    "    recommender.print_recommendations(muscle_recs)\n",
    "\n",
    "    # 5. Similar recipes\n",
    "    print(\"\\n6. RECIPES SIMILAR TO FIRST RECIPE\")\n",
    "    if len(df) > 0:\n",
    "        similar_recs = recommender.recommend_similar_to_recipe(\n",
    "            0,  # First recipe in dataset\n",
    "            n_recommendations=3,\n",
    "            max_calories=800\n",
    "        )\n",
    "        recommender.print_recommendations(similar_recs)\n",
    "\n",
    "    return recommender\n",
    "\n",
    "# Run the demonstration\n",
    "nutrition_recommender = demonstrate_recommendation_system(df)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "03408c1ea3844232b86090142775c212": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_23bed68f646f46528ab2a430a0d9a3d1",
      "placeholder": "​",
      "style": "IPY_MODEL_038837fd0e2446f4917367379ba4a097",
      "value": "MM-Food-100K.csv: 100%"
     }
    },
    "038837fd0e2446f4917367379ba4a097": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "09bd9687ddfa49dab2559e2eaaab3b28": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f19436e3dc9b437fb1458325f8ace551",
      "max": 28598273,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_980c38ef7d0645ec8ac69fd594d87b98",
      "value": 28598273
     }
    },
    "10407668939d4e27a35fa0928d4a8cba": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1eba0ea384c44856814675d639d2355e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "22c478b870f74824adc3451faed40e23": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "23bed68f646f46528ab2a430a0d9a3d1": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4ff5a1ee06734e269860cab268549168": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "55bfcfbd038f4661b3e618ca6f291ab2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_71d55f65e7384705b4d70100adb0b171",
      "placeholder": "​",
      "style": "IPY_MODEL_78bdf17b658243768b34e2ee03aa40dd",
      "value": " 14.6k/? [00:00&lt;00:00, 1.01MB/s]"
     }
    },
    "5cf87ad4159b42c6a120097f905243b3": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "20px"
     }
    },
    "71d55f65e7384705b4d70100adb0b171": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "78bdf17b658243768b34e2ee03aa40dd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "7de47fa048ee427e800506ed0b110899": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7e3366b054cf4df49f94fba40d8f7cd4": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "849bd2ef797d41aabc80797a82a78042": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_03408c1ea3844232b86090142775c212",
       "IPY_MODEL_09bd9687ddfa49dab2559e2eaaab3b28",
       "IPY_MODEL_c98906d9bcd04900945904330a8b0230"
      ],
      "layout": "IPY_MODEL_7e3366b054cf4df49f94fba40d8f7cd4"
     }
    },
    "86a91cc9344845a3a4a0b51b9e170469": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "923164f6eb884ce3884c4a1e038f5254": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "980c38ef7d0645ec8ac69fd594d87b98": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "9ecf43026b3b4b588d944ddacb6d5ebf": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_a4b0015c22a7408587f0b506e446e69b",
       "IPY_MODEL_c69a3f06e35f4bbb813bdb04a37d7543",
       "IPY_MODEL_55bfcfbd038f4661b3e618ca6f291ab2"
      ],
      "layout": "IPY_MODEL_4ff5a1ee06734e269860cab268549168"
     }
    },
    "a4b0015c22a7408587f0b506e446e69b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_22c478b870f74824adc3451faed40e23",
      "placeholder": "​",
      "style": "IPY_MODEL_e8ec358ee5e34958b2e09c5c312b5d09",
      "value": "README.md: "
     }
    },
    "a4b9748c2cb645859fd4e6e376e886b2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_faed6e6c71984294a5a5dc4242e93055",
       "IPY_MODEL_ed8a9a1078c042f48f7ff3186c45a9fe",
       "IPY_MODEL_ef60fe0b89e041089d9545b232935c78"
      ],
      "layout": "IPY_MODEL_be9f5cd2b1724c01891375f54358795c"
     }
    },
    "bdbeaa88c76c4cdea870e56e88124264": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "be9f5cd2b1724c01891375f54358795c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c69a3f06e35f4bbb813bdb04a37d7543": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5cf87ad4159b42c6a120097f905243b3",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_e75501af479b4c2d8ee294c3952ca4cd",
      "value": 1
     }
    },
    "c98906d9bcd04900945904330a8b0230": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7de47fa048ee427e800506ed0b110899",
      "placeholder": "​",
      "style": "IPY_MODEL_1eba0ea384c44856814675d639d2355e",
      "value": " 28.6M/28.6M [00:00&lt;00:00, 38.6MB/s]"
     }
    },
    "e6bb9597e53b4ec5a547ce4f91e19c07": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "e75501af479b4c2d8ee294c3952ca4cd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "e8ec358ee5e34958b2e09c5c312b5d09": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "ed8a9a1078c042f48f7ff3186c45a9fe": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_86a91cc9344845a3a4a0b51b9e170469",
      "max": 100000,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_bdbeaa88c76c4cdea870e56e88124264",
      "value": 100000
     }
    },
    "ef60fe0b89e041089d9545b232935c78": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f9417f455f3740918133d0abf661960f",
      "placeholder": "​",
      "style": "IPY_MODEL_923164f6eb884ce3884c4a1e038f5254",
      "value": " 100000/100000 [00:01&lt;00:00, 73072.26 examples/s]"
     }
    },
    "f19436e3dc9b437fb1458325f8ace551": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f9417f455f3740918133d0abf661960f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "faed6e6c71984294a5a5dc4242e93055": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_10407668939d4e27a35fa0928d4a8cba",
      "placeholder": "​",
      "style": "IPY_MODEL_e6bb9597e53b4ec5a547ce4f91e19c07",
      "value": "Generating train split: 100%"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
