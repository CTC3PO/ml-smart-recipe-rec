# -*- coding: utf-8 -*-
"""recipe-recs-2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1pS8pp--3iFCRtJCJt5gwAO-Qnyh86urO

This code is working up to section 6
- **Version by 08.22.25 (4:30 PM)**

# RECIPE RECOMMENDATION SYSTEM


**PROJECT OBJECTIVE:**
Build an end-to-end machine learning pipeline that transforms raw recipe data into
personalized, intelligent food recommendations supporting diverse dietary needs and health goals.

**Core Capabilities:**
- Multi-modal recipe analysis (nutrition, ingredients, cooking methods, images)
- Personalized recommendations based on dietary restrictions and health goals
- Scalable ML pipeline with feature engineering and model evaluation
- Production-ready modular architecture

**Business Value:**
- Helps users discover recipes aligned with their health and dietary preferences
- Reduces food waste by suggesting recipes based on available ingredients
- Supports meal planning for specific nutritional goals (weight loss, muscle gain, etc.)
- Enables food businesses to provide personalized recommendations

**Technical Approach:**
1. Data Engineering: Clean and standardize 100K+ recipe dataset
2. Feature Engineering: Extract nutritional, dietary, and behavioral features
3. ML Pipeline: Build predictive models for nutrition estimation
4. Recommendation Engine: Hybrid content-based + collaborative filtering
5. Visual Analysis: Extract features from food images
6. Deployment: Web interface for real-time recommendations

**Success Metrics:**
- Recommendation accuracy and user satisfaction
- Model performance (RÂ² score for nutrition prediction)
- System scalability and response time
- Coverage of dietary restrictions and health goals

# redipe-recs 2

## 1.PROJECT SETUP & DATA LOADING


Objective: Load and initially explore the MM-Food-100K dataset
- Load dataset from Hugging Face
- Perform initial data quality assessment
- Set up project environment and dependencies
"""

# Install libraries (only run once)
!pip install torch_geometric
!pip install datasets

# Import necessary libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from datasets import load_dataset
import os
import json
import ast
import re
from collections import Counter
import networkx as nx
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler
from PIL import Image
import requests
from io import BytesIO
import cv2
from collections import Counter
import warnings
warnings.filterwarnings('ignore')


import torch
import torch.nn as nn
import torch.nn.functional as F
from torch_geometric.nn import GATConv, SAGEConv, HeteroConv
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.decomposition import NMF, TruncatedSVD
from sklearn.neighbors import NearestNeighbors
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.preprocessing import StandardScaler
from sklearn.impute import SimpleImputer
import umap

# Set style for visualizations
plt.style.use('seaborn-v0_8')
sns.set_palette("husl")

# Optional advanced imports (with error handling)
try:
    import xgboost as xgb
    XGB_AVAILABLE = True
except ImportError:
    XGB_AVAILABLE = False
    print("XGBoost not available. Install with: pip install xgboost")

def load_and_explore_dataset():
    """Load the MM-Food-100K dataset and perform initial exploration"""
    print("Loading dataset from Hugging Face...")

    # Load the dataset
    dataset = load_dataset("Codatta/MM-Food-100K")

    # Convert to pandas DataFrame for easier manipulation
    df = pd.DataFrame(dataset['train'])

    print(f"Dataset shape: {df.shape}")
    print("\nDataset columns:")
    print(df.columns.tolist())

    # Display basic info
    print("\nDataset info:")
    print(df.info())

    # Display first few rows
    print("\nFirst 5 rows:")
    display(df.head())

    return df

# Load the dataset
df_raw = load_and_explore_dataset()

"""## 2.DATA CLEANING & PREPROCESSING


Objective: Clean and prepare the dataset for analysis
- Handle missing values and outliers
- Standardize data formats
- Extract and transform key features

"""

# Clean nutritional data
def clean_nutritional_data(df):
    """Clean and validate nutritional information"""
    print("Cleaning nutritional data...")

    # Function to extract nutrition values safely
    def extract_nutrition(nutrition_json, key):
        try:
            if pd.isna(nutrition_json):
                return np.nan
            nutrition_dict = json.loads(nutrition_json.replace("'", "\""))
            return nutrition_dict.get(key, np.nan)
        except:
            return np.nan

    # Extract nutritional information from JSON
    nutrition_cols = ['calories_kcal', 'protein_g', 'fat_g', 'carbohydrate_g']
    for col in nutrition_cols:
        df[col] = df['nutritional_profile'].apply(lambda x: extract_nutrition(x, col))

    # Convert to numeric and handle missing values
    for col in nutrition_cols:
        df[col] = pd.to_numeric(df[col], errors='coerce')
        # Fill any remaining NaN with median by food_type
        if 'food_type' in df.columns and df['food_type'].notna().any():
            df[col] = df.groupby('food_type')[col].transform(
                lambda x: x.fillna(x.median()) if x.notnull().sum() > 0 else x.fillna(0)
            )
        else:
             df[col] = df[col].fillna(df[col].median() if df[col].notna().sum() > 0 else 0)


    return df

# Clean ingredient data

def clean_ingredient_data(df):
    """Process and standardize ingredient lists"""
    print("Cleaning ingredient data...")

    def clean_ingredient_list(ingredient_str):
        """Clean and standardize ingredient lists"""
        try:
            if pd.isna(ingredient_str) or ingredient_str == '[]' or ingredient_str is None:
                return []

            # Handle different list formats
            if isinstance(ingredient_str, str):
                # Clean the string
                ingredient_str = ingredient_str.strip()
                if ingredient_str.startswith('[') and ingredient_str.endswith(']'):
                    ingredients = ast.literal_eval(ingredient_str)
                else:
                    # Handle malformed lists
                    ingredients = [ing.strip() for ing in ingredient_str.split(',')]
            elif isinstance(ingredient_str, list):
                 ingredients = ingredient_str
            else:
                 return []


            # Clean each ingredient
            cleaned_ingredients = []
            for ingredient in ingredients:
                if isinstance(ingredient, str):
                    # Standardize formatting
                    ing = ingredient.lower().strip()
                    # Remove common prefixes/suffixes
                    ing = re.sub(r'^\d+\s*', '', ing)  # Remove quantities like "2 "
                    ing = re.sub(r'\s*\(.*\)', '', ing)  # Remove parentheses content
                    ing = re.sub(r'\s*(tbsp|tsp|cup|cups|oz|lb|lbs|g|kg|ml|l)$', '', ing)  # Remove units
                    ing = ing.strip()

                    if ing and len(ing) > 1:  # Filter out empty/single character ingredients
                        cleaned_ingredients.append(ing)

            return list(set(cleaned_ingredients))  # Remove duplicates

        except (ValueError, SyntaxError, TypeError) as e:
            print(f"Error processing ingredients: {e}")
            return []

    df['ingredients_cleaned'] = df['ingredients'].apply(clean_ingredient_list)
    return df

# Clean cooking method data
def clean_cooking_methods(df):
    """Process and standardize cooking methods"""
    print("Cleaning cooking methods...")

    def clean_cooking_methods(method_str):
        """Clean and standardize cooking methods"""
        try:
            if pd.isna(method_str) or method_str is None or method_str == 'unknown':
                return []

            if isinstance(method_str, str):
                methods = [m.strip().lower() for m in method_str.split(',')]
                methods = [m for m in methods if m and m != 'unknown']
                return list(set(methods))  # Remove duplicates
            elif isinstance(method_str, list):
                 methods = method_str
                 methods = [m.strip().lower() for m in methods if isinstance(m, str)]
                 methods = [m for m in methods if m and m != 'unknown']
                 return list(set(methods))
            else:
                return []

        except (AttributeError, TypeError) as e:
            print(f"Error processing cooking methods: {e}")
            return []

    df['cooking_methods_cleaned'] = df['cooking_method'].apply(clean_cooking_methods)
    return df

# Clean portion size data
def clean_portion_sizes(df):
    """Process and standardize portion size information"""
    print("Processing portion sizes...")

    def extract_portion_weights(portion_str):
        """Extract weights from portion size information"""
        try:
            if pd.isna(portion_str) or portion_str is None or portion_str == '[]':
                return [], 0

            if isinstance(portion_str, str):
                if portion_str.startswith('[') and portion_str.endswith(']'):
                    portions = ast.literal_eval(portion_str)
                else:
                    portions = [p.strip() for p in portion_str.split(',')]
            elif isinstance(portion_str, list):
                 portions = portion_str
            else:
                 return [], 0


            weights = []
            total_weight = 0

            for portion in portions:
                if isinstance(portion, str) and ':' in portion:
                    try:
                        # Extract weight value
                        weight_part = portion.split(':')[1].strip()
                        # Remove units and convert to float
                        weight_value = re.sub(r'[^\d.]', '', weight_part)
                        if weight_value:
                            weight_float = float(weight_value)
                            weights.append(weight_float)
                            total_weight += weight_float
                    except (ValueError, IndexError):
                        continue

            return weights, total_weight

        except (ValueError, SyntaxError, TypeError) as e:
            print(f"Error processing portion sizes: {e}")
            return [], 0

    portion_results = df['portion_size'].apply(extract_portion_weights)
    df['portion_weights'] = portion_results.apply(lambda x: x[0])
    df['total_weight_g'] = portion_results.apply(lambda x: x[1])
    return df

# Clean metadata
def clean_metadata(df):
        """
        OBJECTIVE: Clean dish names, food types, and other metadata
        - Standardize dish names (remove special characters, normalize case)
        - Standardize food type categories
        """
        print("\n4. CLEANING METADATA")
        print("-" * 40)

        # Clean dish names
        def clean_dish_name(name):
            if pd.isna(name) or name is None:
                return 'unknown_dish'

            name = str(name).strip().lower()
            # Remove special characters but keep spaces and letters
            name = re.sub(r'[^\w\s-]', '', name)
            name = re.sub(r'\s+', ' ', name)  # Multiple spaces to single space
            return name.strip() if name.strip() else 'unknown_dish'

        df['dish_name'] = df['dish_name'].apply(clean_dish_name)

        # Standardize food types
        def standardize_food_type(food_type):
            if pd.isna(food_type) or food_type is None:
                return 'unknown'

            food_type = str(food_type).lower().strip()

            # Mapping for common variations
            type_mapping = {
                'homemade': ['homemade', 'home made', 'home-made', 'homecook'],
                'restaurant': ['restaurant', 'restaurant food', 'dining'],
                'packaged': ['packaged', 'packaged food', 'processed', 'commercial'],
                'raw': ['raw', 'raw vegetables', 'raw fruits', 'fresh'],
                'unknown': ['unknown', 'other', '']
            }

            for standard_type, variations in type_mapping.items():
                if food_type in variations:
                    return standard_type

            return food_type

        if 'food_type' in df.columns:
            df['food_type_standardized'] = df['food_type'].apply(standardize_food_type)


        print(f"  Metadata cleaning completed")
        return df

# comprehensive EDA cleaning
def comprehensive_data_cleaning(df):
    """
    Comprehensive data cleaning and wrangling pipeline for MM-Food-100K dataset
    """
    print("="*60)
    print("COMPREHENSIVE DATA CLEANING & WRANGLING")
    print("="*60)

    # Create a copy to avoid modifying the original
    df_clean = df.copy()

    # Apply modular cleaning functions and check for None returns
    df_clean = clean_nutritional_data(df_clean)
    if df_clean is None: return None

    df_clean = clean_ingredient_data(df_clean)
    if df_clean is None: return None

    df_clean = clean_cooking_methods(df_clean)
    if df_clean is None: return None

    df_clean = clean_portion_sizes(df_clean)
    if df_clean is None: return None

    df_clean = clean_metadata(df_clean)
    if df_clean is None: return None


    # Additional cleaning steps (keep original code for other analyses)
    # 1. Handle Missing Values (already partially handled in modular functions, could refine here)
    print("\nHandling remaining missing values...")
    # Example: Fill any remaining NaNs in nutritional columns with overall median
    nutrition_cols = ['calories_kcal', 'protein_g', 'fat_g', 'carbohydrate_g', 'total_weight_g']
    for col in nutrition_cols:
        if col in df_clean.columns:
            df_clean[col] = df_clean[col].fillna(df_clean[col].median() if df_clean[col].notna().any() else 0)


    # 2. Outlier Detection and Handling (keep original code)
    print("\nHandling outliers...")
    def detect_outliers_iqr(series, threshold=1.5):
        """Detect outliers using IQR method"""
        if series.empty or series.isna().all():
             return pd.Series(False, index=series.index)
        Q1 = series.quantile(0.25)
        Q3 = series.quantile(0.75)
        IQR = Q3 - Q1
        lower_bound = Q1 - threshold * IQR
        upper_bound = Q3 + threshold * IQR
        return (series < lower_bound) | (series > upper_bound)

    outlier_cols = ['calories_kcal', 'protein_g', 'fat_g', 'carbohydrate_g', 'total_weight_g']
    for col in outlier_cols:
        if col in df_clean.columns:
            # Ensure column is numeric before detecting outliers
            df_clean[col] = pd.to_numeric(df_clean[col], errors='coerce')
            # Cap outliers using 1st and 99th percentiles
            if df_clean[col].notna().sum() > 0: # Check if there are non-NaN values
                lower_bound = df_clean[col].quantile(0.01)
                upper_bound = df_clean[col].quantile(0.99)
                df_clean[col] = df_clean[col].clip(lower_bound, upper_bound)


    # 3. Feature Engineering Preparation (keep original code)
    print("\nPreparing for feature engineering...")
    df_clean['has_nutrition_data'] = (
        (df_clean['calories_kcal'].notna()) &
        (df_clean['protein_g'].notna()) &
        (df_clean['fat_g'].notna()) &
        (df_clean['carbohydrate_g'].notna())
    )

    df_clean['has_ingredients'] = df_clean['ingredients_cleaned'].apply(lambda x: len(x) > 0)
    df_clean['has_cooking_methods'] = df_clean['cooking_methods_cleaned'].apply(lambda x: len(x) > 0)
    df_clean['has_portion_data'] = df_clean['total_weight_g'].notna() & (df_clean['total_weight_g'] > 0)

    df_clean['data_quality_score'] = (
        df_clean['has_nutrition_data'].astype(int) +
        df_clean['has_ingredients'].astype(int) +
        df_clean['has_cooking_methods'].astype(int) +
        df_clean['has_portion_data'].astype(int)
    ) / 4


    # 4. Final Data Quality Report (keep original code)
    print("\nFinal data quality report:")
    quality_report = {
        'total_records': len(df_clean),
        'records_with_complete_nutrition': df_clean['has_nutrition_data'].sum(),
        'records_with_ingredients': df_clean['has_ingredients'].sum(),
        'records_with_cooking_methods': df_clean['has_cooking_methods'].sum(),
        'records_with_portion_data': df_clean['has_portion_data'].sum(),
        'records_with_high_quality': (df_clean['data_quality_score'] >= 0.75).sum(),
        'average_data_quality_score': df_clean['data_quality_score'].mean()
    }
    for metric, value in quality_report.items():
        if 'average' in metric:
            print(f"{metric}: {value:.3f}")
        else:
            print(f"{metric}: {value}")


    # 5. Export cleaned data (keep original code)
    print("\nExporting cleaned data...")
    clean_columns = [
        'dish_name', 'food_type_standardized', 'calories_kcal',
        'protein_g', 'fat_g', 'carbohydrate_g', 'ingredients_cleaned',
        'cooking_methods_cleaned', 'portion_weights', 'total_weight_g',
        'has_nutrition_data', 'has_ingredients', 'has_cooking_methods',
        'has_portion_data', 'data_quality_score'
    ]
    original_clean_cols = [col for col in df.columns if col not in [
        'dish_name', 'food_type', 'nutritional_profile', 'ingredients',
        'cooking_method', 'portion_size'
    ]]
    essential_original_cols = ['sub_dt', 'image_url', 'camera_or_phone_prob', 'food_prob']
    for col in essential_original_cols:
      if col not in original_clean_cols:
        original_clean_cols.append(col)

    final_columns = clean_columns + original_clean_cols
    df_final = df_clean[[col for col in final_columns if col in df_clean.columns]]
    df_final.to_csv('mm_food_100k_cleaned.csv', index=False)
    print("Cleaned dataset saved to 'mm_food_100k_cleaned.csv'")

    return df_final

# Perform comprehensive cleaning
df = comprehensive_data_cleaning(df_raw)


# =============================================================================
# FEATURE ENGINEERING: NUTRITIONAL FEATURES
# Objective: Create nutrition-based features with clear rationale
# =============================================================================

def create_nutritional_features(df):
    """Create nutrition-based features with clear rationale"""
    print("Creating nutritional features...")

    # Make a copy to avoid modifying the original
    df = df.copy()

    # Calculate nutritional ratios
    df['protein_calorie_ratio'] = df['protein_g'] / df['calories_kcal']
    df['fat_calorie_ratio'] = df['fat_g'] / df['calories_kcal']
    df['carb_calorie_ratio'] = df['carbohydrate_g'] / df['calories_kcal']

    # Calculate macronutrient percentages
    total_macros = df['protein_g'] + df['fat_g'] + df['carbohydrate_g']
    df['protein_pct'] = df['protein_g'] / total_macros * 100
    df['fat_pct'] = df['fat_g'] / total_macros * 100
    df['carb_pct'] = df['carbohydrate_g'] / total_macros * 100

    # Energy density
    df['energy_density'] = df['calories_kcal'] / df['total_weight_g'].replace(0, 1)

    # Replace infinities with NaN
    df.replace([np.inf, -np.inf], np.nan, inplace=True)

    print("â Nutritional features created")
    print(f"New columns: {['protein_calorie_ratio', 'fat_calorie_ratio', 'carb_calorie_ratio', 'protein_pct', 'fat_pct', 'carb_pct', 'energy_density']}")

    return df

# Apply nutritional feature engineering
df = create_nutritional_features(df)

# =============================================================================
# FEATURE ENGINEERING: INGREDIENT FEATURES
# Objective: Create ingredient-based features efficiently
# =============================================================================

def create_ingredient_features(df):
    """Create ingredient-based features efficiently"""
    print("Creating ingredient features...")

    # Make a copy to avoid modifying the original
    df = df.copy()

    # Count ingredients
    df['num_ingredients'] = df['ingredients_cleaned'].apply(len)

    # Flag for common dietary patterns
    vegan_restricted = ['meat', 'chicken', 'beef', 'pork', 'fish', 'seafood', 'egg',
                       'dairy', 'milk', 'cheese', 'butter', 'honey', 'gelatin']
    vegetarian_restricted = ['meat', 'beef', 'pork', 'chicken', 'fish', 'seafood']
    gluten_restricted = ['wheat', 'barley', 'rye', 'bread', 'pasta', 'cereal', 'couscous']

    def check_diet_compliance(ingredients, restricted_items):
        if not isinstance(ingredients, list):
            return False
        ingredient_text = ' '.join([str(ing).lower() for ing in ingredients])
        return not any(item in ingredient_text for item in restricted_items)

    df['is_vegan'] = df['ingredients_cleaned'].apply(
        lambda x: check_diet_compliance(x, vegan_restricted)
    )
    df['is_vegetarian'] = df['ingredients_cleaned'].apply(
        lambda x: check_diet_compliance(x, vegetarian_restricted)
    )
    df['is_gluten_free'] = df['ingredients_cleaned'].apply(
        lambda x: check_diet_compliance(x, gluten_restricted)
    )

    print("â Ingredient features created")
    print(f"New columns: {['num_ingredients', 'is_vegan', 'is_vegetarian', 'is_gluten_free']}")

    return df

# Apply ingredient feature engineering
df = create_ingredient_features(df)

# =============================================================================
# FEATURE ENGINEERING: COOKING FEATURES
# Objective: Create cooking method features
# =============================================================================

def create_cooking_features(df):
    """Create cooking method features"""
    print("Creating cooking features...")

    # Make a copy to avoid modifying the original
    df = df.copy()

    # Count cooking methods
    df['num_cooking_methods'] = df['cooking_methods_cleaned'].apply(len)

    # Flag for common cooking techniques
    healthy_methods = ['steam', 'boil', 'bake', 'grill', 'roast']
    unhealthy_methods = ['fry', 'deep fry', 'pan fry']

    def count_cooking_types(methods, method_list):
        if not isinstance(methods, list):
            return 0
        return sum(1 for method in methods if method in method_list)

    df['healthy_cooking_count'] = df['cooking_methods_cleaned'].apply(
        lambda x: count_cooking_types(x, healthy_methods)
    )
    df['unhealthy_cooking_count'] = df['cooking_methods_cleaned'].apply(
        lambda x: count_cooking_types(x, unhealthy_methods)
    )

    print("â Cooking features created")
    print(f"New columns: {['num_cooking_methods', 'healthy_cooking_count', 'unhealthy_cooking_count']}")

    return df

# Apply cooking feature engineering
df = create_cooking_features(df)

# =============================================================================
# FEATURE ENGINEERING: FEATURE IMPORTANCE ANALYSIS
# Objective: Analyze which features matter most
# =============================================================================

def analyze_feature_importance(df, target_col='calories_kcal'):
    """Analyze which features matter most"""
    from sklearn.ensemble import RandomForestRegressor

    print(f"Analyzing feature importance for target: {target_col}")

    # Select numeric features
    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
    if target_col in numeric_cols:
        numeric_cols.remove(target_col)

    print(f"Using {len(numeric_cols)} numeric features for importance analysis")

    # Prepare data
    X = df[numeric_cols].fillna(0)
    y = df[target_col].fillna(0)

    # Train simple model for feature importance
    model = RandomForestRegressor(n_estimators=50, random_state=42)
    model.fit(X, y)

    # Get feature importance
    importance = pd.DataFrame({
        'feature': numeric_cols,
        'importance': model.feature_importances_
    }).sort_values('importance', ascending=False)

    # Plot feature importance
    plt.figure(figsize=(12, 10))
    sns.barplot(x='importance', y='feature', data=importance.head(15))
    plt.title('Top 15 Feature Importance')
    plt.tight_layout()
    plt.show()

    # Display top features
    print("Top 10 most important features:")
    for i, row in importance.head(10).iterrows():
        print(f"  {i+1}. {row['feature']}: {row['importance']:.4f}")

    return importance

# Analyze feature importance
feature_importance = analyze_feature_importance(df)

# =============================================================================
# COMPREHENSIVE FEATURE ENGINEERING
# Objective: Apply all feature engineering steps
# =============================================================================

def comprehensive_feature_engineering(df):
    """Apply all feature engineering steps"""
    print("="*60)
    print("COMPREHENSIVE FEATURE ENGINEERING")
    print("="*60)

    # Track original columns
    original_columns = set(df.columns)

    # Apply all feature engineering steps
    df = create_nutritional_features(df)
    df = create_ingredient_features(df)
    df = create_cooking_features(df)

    # Identify new columns
    new_columns = set(df.columns) - original_columns
    print(f"\nâ Feature engineering complete!")
    print(f"Added {len(new_columns)} new features: {list(new_columns)}")

    return df

# Apply comprehensive feature engineering
df = comprehensive_feature_engineering(df)

# =============================================================================
# FEATURE SUMMARY
# Objective: Display summary of all features
# =============================================================================

def display_feature_summary(df):
    """Display summary of all features"""
    print("="*60)
    print("FEATURE SUMMARY")
    print("="*60)

    # Count features by type
    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
    categorical_cols = df.select_dtypes(include=['object', 'category']).columns.tolist()
    bool_cols = df.select_dtypes(include=['bool']).columns.tolist()
    list_cols = [col for col in df.columns if df[col].apply(lambda x: isinstance(x, list)).any()]

    print(f"Total features: {len(df.columns)}")
    print(f"Numeric features: {len(numeric_cols)}")
    print(f"Categorical features: {len(categorical_cols)}")
    print(f"Boolean features: {len(bool_cols)}")
    print(f"List features: {len(list_cols)}")

    # Show sample of each feature type
    print(f"\nSample numeric features: {numeric_cols[:5]}")
    print(f"Sample categorical features: {categorical_cols[:5]}")
    print(f"Sample boolean features: {bool_cols}")
    print(f"Sample list features: {list_cols}")

    # Show missing values
    missing_values = df.isnull().sum()
    missing_percent = (missing_values / len(df)) * 100
    print(f"\nFeatures with missing values: {len(missing_values[missing_values > 0])}")
    if len(missing_values[missing_values > 0]) > 0:
        print("Top features with missing values:")
        for col, count in missing_values[missing_values > 0].sort_values(ascending=False).head(5).items():
            print(f"  {col}: {count} ({missing_percent[col]:.1f}%)")

# Display feature summary
display_feature_summary(df)

"""## 5.MACHINE LEARNING PIPELINE

Objective: Build and evaluate predictive models
- Prepare features and targets with proper validation
- Train simple baseline models for comparison
- Train more sophisticated models
- Comprehensive model evaluation with multiple metrics
"""

# =============================================================================
# ML PIPELINE: DATA PREPARATION
# Objective: Prepare features and targets with proper validation
# =============================================================================

def prepare_ml_data(df, target_col='calories_kcal', test_size=0.2, random_state=42):
    """Prepare features and targets with proper validation"""
    print("Preparing data for modeling...")

    # Select numeric features
    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
    if target_col in numeric_cols:
        numeric_cols.remove(target_col)

    print(f"Using {len(numeric_cols)} numeric features for modeling")
    print(f"Target variable: {target_col}")

    # Prepare features and target
    X = df[numeric_cols].fillna(0)
    y = df[target_col].fillna(0)

    # Train-test split
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=test_size, random_state=random_state
    )

    print(f"Training set: {X_train.shape[0]} samples")
    print(f"Test set: {X_test.shape[0]} samples")

    # Scale features
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)

    # Store results in a dictionary
    ml_data = {
        'X_train': X_train_scaled,
        'X_test': X_test_scaled,
        'y_train': y_train,
        'y_test': y_test,
        'scaler': scaler,
        'feature_names': numeric_cols,
        'target_col': target_col
    }

    print("â Data preparation complete!")
    return ml_data

# Prepare the data
ml_data = prepare_ml_data(df, target_col='calories_kcal')

# =============================================================================
# ML PIPELINE: BASELINE MODELS
# Objective: Train simple baseline models for comparison
# =============================================================================

def train_baseline_models(ml_data):
    """Train simple baseline models for comparison"""
    print("Training baseline models...")

    from sklearn.linear_model import LinearRegression
    from sklearn.ensemble import RandomForestRegressor
    from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

    X_train, X_test, y_train, y_test = ml_data['X_train'], ml_data['X_test'], ml_data['y_train'], ml_data['y_test']
    models = {}

    # Linear Regression
    print("Training Linear Regression...")
    lr = LinearRegression()
    lr.fit(X_train, y_train)
    lr_pred = lr.predict(X_test)

    lr_metrics = {
        'mae': mean_absolute_error(y_test, lr_pred),
        'mse': mean_squared_error(y_test, lr_pred),
        'rmse': np.sqrt(mean_squared_error(y_test, lr_pred)),
        'r2': r2_score(y_test, lr_pred)
    }

    models['Linear Regression'] = {'model': lr, 'metrics': lr_metrics}
    print(f"  Linear Regression RÂ²: {lr_metrics['r2']:.4f}")

    # Random Forest
    print("Training Random Forest...")
    rf = RandomForestRegressor(n_estimators=50, random_state=42, n_jobs=-1)
    rf.fit(X_train, y_train)
    rf_pred = rf.predict(X_test)

    rf_metrics = {
        'mae': mean_absolute_error(y_test, rf_pred),
        'mse': mean_squared_error(y_test, rf_pred),
        'rmse': np.sqrt(mean_squared_error(y_test, rf_pred)),
        'r2': r2_score(y_test, rf_pred)
    }

    models['Random Forest'] = {'model': rf, 'metrics': rf_metrics}
    print(f"  Random Forest RÂ²: {rf_metrics['r2']:.4f}")

    # Add models to ml_data
    ml_data['models'] = models
    ml_data['baseline_models'] = list(models.keys())

    print("â Baseline models training complete!")
    return ml_data

# Train baseline models
ml_data = train_baseline_models(ml_data)

# =============================================================================
# ML PIPELINE: ADVANCED MODELS
# Objective: Train more sophisticated models
# =============================================================================

def train_advanced_models(ml_data):
    """Train more sophisticated models"""
    print("Training advanced models...")

    from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

    X_train, X_test, y_train, y_test = ml_data['X_train'], ml_data['X_test'], ml_data['y_train'], ml_data['y_test']

    # XGBoost if available
    if XGB_AVAILABLE:
        print("Training XGBoost...")
        xgb_model = xgb.XGBRegressor(n_estimators=50, random_state=42)
        xgb_model.fit(X_train, y_train)
        xgb_pred = xgb_model.predict(X_test)

        xgb_metrics = {
            'mae': mean_absolute_error(y_test, xgb_pred),
            'mse': mean_squared_error(y_test, xgb_pred),
            'rmse': np.sqrt(mean_squared_error(y_test, xgb_pred)),
            'r2': r2_score(y_test, xgb_pred)
        }

        ml_data['models']['XGBoost'] = {'model': xgb_model, 'metrics': xgb_metrics}
        print(f"  XGBoost RÂ²: {xgb_metrics['r2']:.4f}")
    else:
        print("XGBoost not available. Install with: pip install xgboost")

    print("â Advanced models training complete!")
    return ml_data

# Train advanced models
ml_data = train_advanced_models(ml_data)

# =============================================================================
# ML PIPELINE: MODEL EVALUATION
# Objective: Comprehensive model evaluation with multiple metrics
# =============================================================================

def evaluate_models(ml_data):
    """Comprehensive model evaluation with multiple metrics"""
    print("Evaluating models...")

    # Create comparison table
    comparison_data = []
    for name, result in ml_data['models'].items():
        comparison_data.append({
            'Model': name,
            'RÂ² Score': result['metrics']['r2'],
            'MAE': result['metrics']['mae'],
            'RMSE': result['metrics']['rmse']
        })

    comparison_df = pd.DataFrame(comparison_data).sort_values('RÂ² Score', ascending=False)
    print("Model Performance Comparison:")
    print(comparison_df.to_string(index=False))

    # Find best model
    best_model_name = comparison_df.iloc[0]['Model']
    best_model = ml_data['models'][best_model_name]['model']
    best_metrics = ml_data['models'][best_model_name]['metrics']

    print(f"\nBest model: {best_model_name} (RÂ²: {best_metrics['r2']:.4f})")

    # Plot predictions vs actual
    best_predictions = best_model.predict(ml_data['X_test'])

    plt.figure(figsize=(10, 6))
    plt.scatter(ml_data['y_test'], best_predictions, alpha=0.6)
    plt.plot([ml_data['y_test'].min(), ml_data['y_test'].max()],
             [ml_data['y_test'].min(), ml_data['y_test'].max()], 'r--', lw=2)
    plt.xlabel('Actual Values')
    plt.ylabel('Predicted Values')
    plt.title(f'{best_model_name} - Actual vs Predicted')
    plt.show()

    # Add to ml_data
    ml_data['comparison_df'] = comparison_df
    ml_data['best_model_name'] = best_model_name
    ml_data['best_model'] = best_model
    ml_data['best_metrics'] = best_metrics

    print("â Model evaluation complete!")
    return ml_data

# Evaluate models
ml_data = evaluate_models(ml_data)

# =============================================================================
# COMPREHENSIVE ML PIPELINE
# Objective: Run complete machine learning pipeline
# =============================================================================

def run_comprehensive_ml_pipeline(df, target_col='calories_kcal'):
    """Run complete machine learning pipeline"""
    print("="*60)
    print("COMPREHENSIVE MACHINE LEARNING PIPELINE")
    print("="*60)

    # Run all steps
    ml_data = prepare_ml_data(df, target_col=target_col)
    ml_data = train_baseline_models(ml_data)
    ml_data = train_advanced_models(ml_data)
    ml_data = evaluate_models(ml_data)
    ml_data = analyze_model_feature_importance(ml_data)

    # Display final results
    print("\n" + "="*60)
    print("PIPELINE COMPLETE - FINAL RESULTS")
    print("="*60)

    best_model_name = ml_data['best_model_name']
    best_metrics = ml_data['best_metrics']

    print(f"Best Model: {best_model_name}")
    print(f"RÂ² Score: {best_metrics['r2']:.4f}")
    print(f"MAE: {best_metrics['mae']:.2f}")
    print(f"RMSE: {best_metrics['rmse']:.2f}")

    print(f"\nModels trained: {list(ml_data['models'].keys())}")
    print(f"Features used: {len(ml_data['feature_names'])}")

    return ml_data

# Run comprehensive ML pipeline
ml_results = run_comprehensive_ml_pipeline(df, target_col='calories_kcal')

"""## 6.RECOMMENDATION SYSTEM
Objective: Build intelligent recipe recommendation engine
- Improved content-based filtering
- Add collaborative filtering capabilities
- Combine multiple recommendation approaches

### method 1 (old)
**working**
"""

class EfficientNutritionalRecommender:
    """
    Memory-efficient recommendation system that won't crash Colab
    """

    def __init__(self, df, max_samples=100000, max_ingredients=500):
        """
        Initialize with memory constraints

        Args:
            df: DataFrame with recipe data
            max_samples: Maximum number of recipes to use (prevents memory issues)
            max_ingredients: Maximum ingredients to consider for similarity
        """
        print(f"Initializing Efficient Recommendation System...")

        # Sample data to prevent memory issues
        if len(df) > max_samples:
            print(f"Sampling {max_samples} recipes from {len(df)} total recipes")
            self.df = df.sample(n=max_samples, random_state=42).reset_index(drop=True)
        else:
            self.df = df.copy().reset_index(drop=True)

        self.max_ingredients = max_ingredients
        self._prepare_efficient_features()
        print(f"Recommender initialized with {len(self.df)} recipes")

    def _prepare_efficient_features(self):
        """Prepare lightweight features instead of full one-hot encoding"""
        # Get top ingredients only (limits feature space dramatically)
        all_ingredients = []
        for ingredients in self.df['ingredients_cleaned']:
            if isinstance(ingredients, list):
                all_ingredients.extend([ing.lower().strip() for ing in ingredients])

        from collections import Counter
        ingredient_counter = Counter(all_ingredients)
        self.top_ingredients = [ing for ing, count in ingredient_counter.most_common(self.max_ingredients)]

        print(f"Using top {len(self.top_ingredients)} ingredients for recommendations")

        # Create simple dietary flags
        self._create_dietary_flags()

        # Pre-calculate nutritional scores
        self._calculate_nutritional_scores()

    def _create_dietary_flags(self):
        """Create dietary restriction flags efficiently"""
        dietary_restrictions = {
            'vegan_restricted': ['meat', 'beef', 'pork', 'chicken', 'fish', 'seafood', 'shrimp',
                   'salmon', 'tuna', 'crab', 'lobster', 'egg', 'dairy', 'milk',
                   'cheese', 'butter', 'honey', 'gelatin'],
            'vegetarian_restricted': ['meat', 'beef', 'pork', 'chicken', 'fish', 'seafood'],
            'keto_friendly': ['low carb', 'sugar free', 'keto'],
            'gluten_restricted': ['wheat', 'flour', 'bread', 'pasta', 'gluten']
        }

        '''def check_dietary_compliance(ingredients, restricted_items):
            if not isinstance(ingredients, list):
                return False
            ingredient_text = ' '.join([str(ing).lower() for ing in ingredients])
            return not any(item in ingredient_text for item in restricted_items)'''

        def check_dietary_compliance(ingredients, restricted_items):
          if not isinstance(ingredients, list) or not ingredients:
            return True
          ingredient_text = ' '.join([str(ing).lower() for ing in ingredients])
          return not any(item in ingredient_text for item in restricted_items)

        # Create flags
        self.df['is_vegan'] = self.df['ingredients_cleaned'].apply(
            lambda x: check_dietary_compliance(x, dietary_restrictions['vegan_restricted'])
        )
        self.df['is_vegetarian'] = self.df['ingredients_cleaned'].apply(
            lambda x: check_dietary_compliance(x, dietary_restrictions['vegetarian_restricted'])
        )
        self.df['is_gluten_free'] = self.df['ingredients_cleaned'].apply(
            lambda x: check_dietary_compliance(x, dietary_restrictions['gluten_restricted'])
        )

        # High protein flag
        self.df['is_high_protein'] = self.df['protein_g'] > self.df['protein_g'].quantile(0.7)

        # Low calorie flag
        self.df['is_low_calorie'] = self.df['calories_kcal'] < self.df['calories_kcal'].quantile(0.3)

    def _calculate_nutritional_scores(self):
        """Pre-calculate scoring metrics"""
        # Nutritional density score
        if 'nutritional_density' not in self.df.columns:
            self.df['nutritional_density'] = (
                self.df['protein_g'] / self.df['calories_kcal'] * 100
            ).fillna(0)

        # Health score based on multiple factors
        self.df['health_score'] = (
            (self.df['protein_g'] / self.df['calories_kcal'] * 10).fillna(0) +
            self.df['is_high_protein'].astype(int) * 2 +
            self.df['is_low_calorie'].astype(int) * 1
        )

    def recommend_by_diet(self, diet_type, n_recommendations=5, **filters):
        """
        Efficient diet-based recommendations
        """
        diet_filters = {
            'vegan': 'is_vegan',
            'vegetarian': 'is_vegetarian',
            'gluten_free': 'is_gluten_free',
            'high_protein': 'is_high_protein',
            'low_calorie': 'is_low_calorie'
        }

        if diet_type not in diet_filters:
            available_diets = list(diet_filters.keys())
            print(f"Diet type '{diet_type}' not supported. Available: {available_diets}")
            return None

        # Filter by diet
        diet_column = diet_filters[diet_type]
        filtered_df = self.df[self.df[diet_column] == True].copy()

        if len(filtered_df) == 0:
            print(f"No {diet_type} recipes found")
            return self._get_fallback_recommendations(n_recommendations)

        # Apply additional filters
        filtered_df = self._apply_filters(filtered_df, **filters)

        if len(filtered_df) == 0:
            print(f"No {diet_type} recipes match your criteria")
            return self._get_fallback_recommendations(n_recommendations)

        # Sort by health score and nutritional density
        recommendations = filtered_df.nlargest(n_recommendations, ['health_score', 'nutritional_density'])

        return self._format_recommendations(recommendations, f"{diet_type.title()} Recommendations")

    def recommend_by_ingredients(self, preferred_ingredients, excluded_ingredients=None, n_recommendations=5, **filters):
        """
        Efficient ingredient-based recommendations
        """
        if excluded_ingredients is None:
            excluded_ingredients = []

        preferred_ingredients = [ing.lower().strip() for ing in preferred_ingredients]
        excluded_ingredients = [ing.lower().strip() for ing in excluded_ingredients]

        # Score recipes by ingredient matches
        def calculate_ingredient_score(ingredients):
            if not isinstance(ingredients, list):
                return 0

            ingredient_text = ' '.join([str(ing).lower() for ing in ingredients])

            # Calculate preference score
            preference_score = sum(2 for pref in preferred_ingredients if pref in ingredient_text)

            # Penalize for excluded ingredients
            exclusion_penalty = sum(5 for excl in excluded_ingredients if excl in ingredient_text)

            return max(0, preference_score - exclusion_penalty)

        self.df['ingredient_score'] = self.df['ingredients_cleaned'].apply(calculate_ingredient_score)

        # Get recipes with positive scores
        filtered_df = self.df[self.df['ingredient_score'] > 0].copy()

        if len(filtered_df) == 0:
            print("No recipes found matching your ingredient preferences")
            return self._get_fallback_recommendations(n_recommendations)

        # Apply additional filters
        filtered_df = self._apply_filters(filtered_df, **filters)

        if len(filtered_df) == 0:
            print("No recipes match your ingredient preferences and filters")
            return self._get_fallback_recommendations(n_recommendations)

        # Sort by ingredient score and health score
        recommendations = filtered_df.nlargest(n_recommendations, ['ingredient_score', 'health_score'])

        return self._format_recommendations(recommendations, "Ingredient-Based Recommendations")

    def recommend_by_nutrition_goals(self, goal, target_calories=None, n_recommendations=5, **filters):
        """
        Nutrition goal-based recommendations
        """
        goal_configs = {
            'weight_loss': {
                'max_calories': 400,
                'min_protein': 15,
                'sort_by': ['is_low_calorie', 'health_score'],
                'ascending': [False, False]
            },
            'muscle_gain': {
                'min_protein': 20,
                'min_calories': 300,
                'sort_by': ['is_high_protein', 'protein_g'],
                'ascending': [False, False]
            },
            'healthy_eating': {
                'min_protein': 10,
                'sort_by': ['health_score', 'nutritional_density'],
                'ascending': [False, False]
            }
        }

        if goal not in goal_configs:
            available_goals = list(goal_configs.keys())
            print(f"Goal '{goal}' not supported. Available: {available_goals}")
            return None

        config = goal_configs[goal]
        filtered_df = self.df.copy()

        # Apply goal-specific filters
        if 'max_calories' in config:
            filtered_df = filtered_df[filtered_df['calories_kcal'] <= config['max_calories']]

        if 'min_calories' in config:
            filtered_df = filtered_df[filtered_df['calories_kcal'] >= config['min_calories']]

        if 'min_protein' in config:
            filtered_df = filtered_df[filtered_df['protein_g'] >= config['min_protein']]

        # Apply target calories if specified
        if target_calories:
            calorie_range = target_calories * 0.2  # 20% tolerance
            filtered_df = filtered_df[
                (filtered_df['calories_kcal'] >= target_calories - calorie_range) &
                (filtered_df['calories_kcal'] <= target_calories + calorie_range)
            ]

        # Apply additional filters
        filtered_df = self._apply_filters(filtered_df, **filters)

        if len(filtered_df) == 0:
            print(f"No recipes found for {goal} goal")
            return self._get_fallback_recommendations(n_recommendations)

        # Sort according to goal configuration
        recommendations = filtered_df.sort_values(
            config['sort_by'],
            ascending=config['ascending']
        ).head(n_recommendations)

        return self._format_recommendations(recommendations, f"{goal.replace('_', ' ').title()} Recommendations")

    def recommend_similar_recipes(self, recipe_index, n_recommendations=5, **filters):
        """
        Find similar recipes using simplified similarity calculation
        """
        if recipe_index >= len(self.df) or recipe_index < 0:
            print("Invalid recipe index")
            return None

        target_recipe = self.df.iloc[recipe_index]

        # Calculate simple similarity based on nutritional profile
        def calculate_similarity(row):
            # Nutritional similarity (normalized)
            nutrition_diff = abs(
                (row['calories_kcal'] - target_recipe['calories_kcal']) / max(target_recipe['calories_kcal'], 1) +
                (row['protein_g'] - target_recipe['protein_g']) / max(target_recipe['protein_g'], 1) +
                (row['fat_g'] - target_recipe['fat_g']) / max(target_recipe['fat_g'], 1)
            )

            # Ingredient overlap (simplified)
            if isinstance(row['ingredients_cleaned'], list) and isinstance(target_recipe['ingredients_cleaned'], list):
                target_ingredients = set([ing.lower() for ing in target_recipe['ingredients_cleaned']])
                recipe_ingredients = set([ing.lower() for ing in row['ingredients_cleaned']])

                if len(target_ingredients) > 0:
                    ingredient_overlap = len(target_ingredients.intersection(recipe_ingredients)) / len(target_ingredients)
                else:
                    ingredient_overlap = 0
            else:
                ingredient_overlap = 0

            # Combined similarity (lower is more similar)
            return nutrition_diff - (ingredient_overlap * 0.5)

        # Calculate similarities
        similarities = self.df.apply(calculate_similarity, axis=1)

        # Get most similar recipes (excluding the target recipe itself)
        similar_indices = similarities.nsmallest(n_recommendations + 1).index[1:]  # Skip first (self)
        similar_recipes = self.df.loc[similar_indices].copy()

        # Apply additional filters
        similar_recipes = self._apply_filters(similar_recipes, **filters)

        if len(similar_recipes) == 0:
            print("No similar recipes found matching your criteria")
            return self._get_fallback_recommendations(n_recommendations)

        return self._format_recommendations(similar_recipes.head(n_recommendations), "Similar Recipes")

    def _apply_filters(self, df, **filters):
        """Apply additional filters efficiently"""
        filtered_df = df.copy()

        # Calorie filters
        if 'min_calories' in filters:
            filtered_df = filtered_df[filtered_df['calories_kcal'] >= filters['min_calories']]
        if 'max_calories' in filters:
            filtered_df = filtered_df[filtered_df['calories_kcal'] <= filters['max_calories']]

        # Protein filters
        if 'min_protein' in filters:
            filtered_df = filtered_df[filtered_df['protein_g'] >= filters['min_protein']]
        if 'max_protein' in filters:
            filtered_df = filtered_df[filtered_df['protein_g'] <= filters['max_protein']]

        # Food type filter
        if 'food_type' in filters and 'food_type_standardized' in filtered_df.columns:
            filtered_df = filtered_df[filtered_df['food_type_standardized'] == filters['food_type']]

        return filtered_df

    def _get_fallback_recommendations(self, n_recommendations):
        """Provide fallback recommendations"""
        print("Showing general healthy recommendations as fallback")
        fallback = self.df.nlargest(n_recommendations, 'health_score')
        return self._format_recommendations(fallback, "General Recommendations (Fallback)")

    def _format_recommendations(self, recommendations, title):
        """Format recommendations for display"""
        results = []

        for idx, (_, recipe) in enumerate(recommendations.iterrows()):
            result = {
                'rank': idx + 1,
                'recipe_id': recipe.name,
                'name': recipe.get('dish_name', 'Unknown Dish'),
                'calories': round(recipe.get('calories_kcal', 0), 1),
                'protein': round(recipe.get('protein_g', 0), 1),
                'carbs': round(recipe.get('carbohydrate_g', 0), 1),
                'fat': round(recipe.get('fat_g', 0), 1),
                'health_score': round(recipe.get('health_score', 0), 2),
                'ingredients': recipe.get('ingredients_cleaned', [])[:5],  # First 5 ingredients
                'cooking_methods': recipe.get('cooking_methods_cleaned', []),
                'is_vegan': recipe.get('is_vegan', False),
                'is_vegetarian': recipe.get('is_vegetarian', False),
                'is_high_protein': recipe.get('is_high_protein', False)
            }
            results.append(result)

        return {
            'title': title,
            'count': len(results),
            'recommendations': results
        }

    def print_recommendations(self, recommendations):
        """Print recommendations in a readable format"""
        if not recommendations:
            print("No recommendations to display")
            return

        print(f"\n{'='*60}")
        print(f"{recommendations['title']}")
        print(f"{'='*60}")
        print(f"Found {recommendations['count']} recommendations\n")

        for rec in recommendations['recommendations']:
            print(f"{rec['rank']}. {rec['name']}")
            print(f"   Calories: {rec['calories']} kcal | Protein: {rec['protein']}g")
            print(f"   Carbs: {rec['carbs']}g | Fat: {rec['fat']}g")
            print(f"   Health Score: {rec['health_score']}")

            # Dietary flags
            flags = []
            if rec['is_vegan']: flags.append('Vegan')
            if rec['is_vegetarian']: flags.append('Vegetarian')
            if rec['is_high_protein']: flags.append('High Protein')
            if flags:
                print(f"   Tags: {', '.join(flags)}")

            print(f"   Ingredients: {', '.join(rec['ingredients'])}")
            if rec['cooking_methods']:
                print(f"   Cooking: {', '.join(rec['cooking_methods'])}")
            print()

# Demonstration function
def demonstrate_efficient_recommender(df):
    """Demonstrate the efficient recommender system"""
    print("Initializing Efficient Recommendation System...")

    try:
        # Initialize with reasonable sample size
        recommender = EfficientNutritionalRecommender(df, max_samples=25000, max_ingredients=200)

        print("\n" + "="*60)
        print("EFFICIENT RECOMMENDATION SYSTEM DEMO")
        print("="*60)

        # 1. Diet-based recommendations
        print("\n1. VEGAN RECOMMENDATIONS")
        vegan_recs = recommender.recommend_by_diet('vegan', n_recommendations=3)
        if vegan_recs:
            recommender.print_recommendations(vegan_recs)

        # 2. Ingredient-based recommendations
        print("\n2. CHICKEN-BASED RECIPES (excluding dairy)")
        ingredient_recs = recommender.recommend_by_ingredients(
            preferred_ingredients=['chicken', 'vegetable'],
            excluded_ingredients=['cheese', 'cream'],
            n_recommendations=3
        )
        if ingredient_recs:
            recommender.print_recommendations(ingredient_recs)

        # 3. Nutrition goal recommendations
        print("\n3. WEIGHT LOSS RECOMMENDATIONS")
        weight_loss_recs = recommender.recommend_by_nutrition_goals('weight_loss', n_recommendations=3)
        if weight_loss_recs:
            recommender.print_recommendations(weight_loss_recs)

        # 4. Similar recipes
        if len(recommender.df) > 0:
            print("\n4. RECIPES SIMILAR TO FIRST RECIPE")
            similar_recs = recommender.recommend_similar_recipes(0, n_recommendations=3)
            if similar_recs:
                recommender.print_recommendations(similar_recs)

        return recommender

    except Exception as e:
        print(f"Error in recommendation system: {str(e)}")
        print("This may be due to missing columns or data formatting issues")
        return None

recommender = demonstrate_efficient_recommender(df)


class SimpleImageProcessor:
    """Simplified image processing with better error handling"""

    def __init__(self, df):
        self.df = df
        self.processed_images = []

    def download_sample_images(self, n_samples=50):
        """Download a small sample of images for demonstration"""
        print(f"Downloading {n_samples} sample images...")

        # Get recipes with image URLs
        recipes_with_images = self.df.dropna(subset=['image_url']).copy()
        if len(recipes_with_images) == 0:
            print("No recipes with image URLs found!")
            return []

        # Sample recipes for processing
        sample_recipes = recipes_with_images.sample(min(n_samples, len(recipes_with_images)), random_state=42)

        successful_downloads = 0
        image_data = []

        for idx, (recipe_idx, recipe) in enumerate(sample_recipes.iterrows()):
            try:
                # Download image with timeout
                response = requests.get(recipe['image_url'], timeout=10)
                response.raise_for_status()

                # Use PIL's Image class directly (avoid naming conflicts)
                from PIL import Image as PILImage
                img = PILImage.open(BytesIO(response.content))

                # Extract basic features
                features = self._extract_basic_features(img)

                image_info = {
                    'recipe_id': recipe_idx,
                    'recipe_name': recipe.get('dish_name', 'Unknown'),
                    'image_url': recipe['image_url'],
                    'width': img.width,
                    'height': img.height,
                    'format': img.format,
                    'features': features,
                    'calories': recipe.get('calories_kcal', 0),
                    'protein': recipe.get('protein_g', 0)
                }

                image_data.append(image_info)
                successful_downloads += 1

                # Close the image to free memory
                img.close()

            except Exception as e:
                # Only show first few errors
                if idx < 3:
                    print(f"Failed to download {recipe['image_url']}: {str(e)}")
                continue

        self.processed_images = image_data
        print(f"Successfully downloaded {successful_downloads}/{len(sample_recipes)} images")
        return image_data

    def _extract_basic_features(self, img):
        """Extract basic visual features without complex processing"""
        # Resize for consistent processing
        img_small = img.resize((50, 50))

        # Convert to numpy array
        img_array = np.array(img_small)

        features = {}

        # Basic color statistics
        if len(img_array.shape) == 3:  # Color image
            features['avg_red'] = float(np.mean(img_array[:, :, 0]))
            features['avg_green'] = float(np.mean(img_array[:, :, 1]))
            features['avg_blue'] = float(np.mean(img_array[:, :, 2]))
            features['is_color'] = True
        else:  # Grayscale
            features['avg_brightness'] = float(np.mean(img_array))
            features['is_color'] = False

        # Basic image properties
        features['aspect_ratio'] = img.width / img.height
        features['total_pixels'] = img.width * img.height

        return features

    def display_sample_images(self, max_display=10):
        """Display sample images with basic information"""
        if not self.processed_images:
            print("No images to display!")
            return

        print(f"\nDISPLAYING {min(len(self.processed_images), max_display)} SAMPLE IMAGES:")
        print("=" * 50)

        for i, img_data in enumerate(self.processed_images[:max_display]):
            print(f"\n{i+1}. {img_data['recipe_name']}")
            print(f"   Calories: {img_data['calories']:.0f} kcal | Protein: {img_data['protein']:.1f}g")
            print(f"   Dimensions: {img_data['width']}x{img_data['height']} px")
            print(f"   Image URL: {img_data['image_url']}")

            # Display basic features
            if 'avg_red' in img_data['features']:
                print(f"   Color - R: {img_data['features']['avg_red']:.1f}, "
                      f"G: {img_data['features']['avg_green']:.1f}, "
                      f"B: {img_data['features']['avg_blue']:.1f}")
            else:
                print(f"   Brightness: {img_data['features']['avg_brightness']:.1f}")

    def create_visual_features_df(self):
        """Create a DataFrame with visual features for ML"""
        if not self.processed_images:
            print("No processed images available!")
            return pd.DataFrame()

        # Create a list of feature dictionaries
        feature_dicts = []
        for img_data in self.processed_images:
            features = {
                'recipe_id': img_data['recipe_id'],
                'has_image': True,
                'image_width': img_data['width'],
                'image_height': img_data['height'],
                'image_aspect_ratio': img_data['features']['aspect_ratio']
            }

            # Add color features if available
            if 'avg_red' in img_data['features']:
                features.update({
                    'avg_red': img_data['features']['avg_red'],
                    'avg_green': img_data['features']['avg_green'],
                    'avg_blue': img_data['features']['avg_blue']
                })
            else:
                features['avg_brightness'] = img_data['features']['avg_brightness']

            feature_dicts.append(features)

        return pd.DataFrame(feature_dicts)

# Simple demonstration function
def demonstrate_simple_image_processing(df):
    """Demonstrate simple image processing"""
    print("Starting Simple Image Processing...")
    print("=" * 40)

    try:
        image_processor = SimpleImageProcessor(df)

        # Process a small sample of images
        processed_images = image_processor.download_sample_images(n_samples=8)

        if processed_images:
            # Display the results
            image_processor.display_sample_images()

            # Create features DataFrame
            visual_features_df = image_processor.create_visual_features_df()
            print(f"\nCreated visual features for {len(visual_features_df)} images")

            return image_processor, visual_features_df
        else:
            print("No images were successfully processed!")
            return None, pd.DataFrame()

    except Exception as e:
        print(f"Error in image processing: {str(e)}")
        import traceback
        traceback.print_exc()
        return None, pd.DataFrame()

# Run the simplified image processing
image_processor, visual_features_df = demonstrate_simple_image_processing(df)

image_processor = demonstrate_enhanced_image_processing(df)

# Integrate visual features with recommendations
if image_processor and recommender:
    enhanced_recommender = image_processor.integrate_with_recommendations(recommender)

    print("\nTESTING VISUALLY-ENHANCED RECOMMENDATIONS:")
    enhanced_recommender.recommend_with_explanation(
        'diet',
        diet_type='vegetarian', n_recommendations=3
   )

class ImageProcessor:
    def __init__(self, df):
        self.df = df

    def download_sample_images(self, n_samples=100, image_url_col='image_url'):
        """Download representative images for analysis"""
        print(f"Downloading {n_samples} sample images...")

        # Sample images to process
        sample_df = self.df.dropna(subset=[image_url_col]).sample(n_samples, random_state=42)
        image_data = []

        for idx, row in sample_df.iterrows():
            try:
                response = requests.get(row[image_url_col], timeout=10)
                response.raise_for_status()
                img = Image.open(BytesIO(response.content))

                image_data.append({
                    'recipe_id': idx,
                    'image_url': row[image_url_col],
                    'image': img,
                    'width': img.width,
                    'height': img.height,
                    'format': img.format,
                    'mode': img.mode
                })

                # Remove img.close() to keep the image open for feature extraction
                # img.close()

            except Exception as e:
                print(f"Failed to download image {row[image_url_col]}: {str(e)}")

        self.sample_images = image_data
        return image_data

    def extract_image_features(self):
        """Extract visual features using pre-trained models"""
        print("Extracting image features...")

        # Placeholder for actual feature extraction
        # In production, you would use a pre-trained CNN

        for img_data in self.sample_images:
            img = img_data['image']

            # Simple feature extraction (replace with actual model)
            img_small = img.resize((50, 50))
            pixels = np.array(img_small).flatten()

            # Basic statistics as placeholder features
            img_data['features'] = {
                'mean_color': np.mean(pixels),
                'std_color': np.std(pixels),
                'min_color': np.min(pixels),
                'max_color': np.max(pixels),
                'histogram': np.histogram(pixels, bins=10)[0].tolist()
            }

        return self.sample_images

    def create_visual_features(self):
        """Create features from image analysis"""
        print("Creating visual features...")

        # Add image features to main dataframe
        for img_data in self.sample_images:
            recipe_id = img_data['recipe_id']
            if recipe_id in self.df.index:
                for feature_name, feature_value in img_data['features'].items():
                    # Ensure list features are handled (e.g., histogram)
                    if isinstance(feature_value, list):
                        # Convert list to a string representation or process differently
                        # For simplicity, we'll store as string for now. A better approach
                        # would be to create separate columns for each histogram bin.
                        self.df.loc[recipe_id, f'image_{feature_name}'] = str(feature_value)
                    else:
                        self.df.loc[recipe_id, f'image_{feature_name}'] = feature_value

        # Fill missing values
        image_feature_cols = [col for col in self.df.columns if col.startswith('image_')]
        for col in image_feature_cols:
            # Use a more appropriate fill strategy for potentially non-numeric data
            if self.df[col].dtype == 'object': # Handle string representations of lists
                 self.df[col] = self.df[col].fillna('[]') # Fill missing lists with empty list string
            else: # Handle numeric features
                self.df[col] = self.df[col].fillna(self.df[col].median())

        return self.df

# Initialize and run image processing
image_processor = ImageProcessor(df)
sample_images = image_processor.download_sample_images(n_samples=50)
image_features = image_processor.extract_image_features()
df = image_processor.create_visual_features()